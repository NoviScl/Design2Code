<!DOCTYPE html><html lang="en"><head>
<style>
/* latin-ext */
@font-face {
  font-family: 'Lato';
  font-style: italic;
  font-weight: 400;
  src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
  unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
  font-family: 'Lato';
  font-style: italic;
  font-weight: 400;
  src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
  font-family: 'Lato';
  font-style: italic;
  font-weight: 700;
  src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
  unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
  font-family: 'Lato';
  font-style: italic;
  font-weight: 700;
  src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
  font-family: 'Lato';
  font-style: normal;
  font-weight: 400;
  src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
  unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
  font-family: 'Lato';
  font-style: normal;
  font-weight: 400;
  src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
  font-family: 'Lato';
  font-style: normal;
  font-weight: 700;
  src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
  unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
  font-family: 'Lato';
  font-style: normal;
  font-weight: 700;
  src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

a {
  color: #1772d0;
  text-decoration: none;
}

a:focus,
a:hover {
  color: #f09228;
  text-decoration: none;
}

body,
td,
th,
tr,
p,
a {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 14px
}

strong {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 14px;
}

heading {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 22px;
}

papertitle {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 14px;
  font-weight: 700
}

name {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 32px;
}

.one {
  width: 160px;
  height: 160px;
  position: relative;
}

.two {
  width: 160px;
  height: 160px;
  position: absolute;
  transition: opacity .2s ease-in-out;
  -moz-transition: opacity .2s ease-in-out;
  -webkit-transition: opacity .2s ease-in-out;
}

.fade {
  transition: opacity .2s ease-in-out;
  -moz-transition: opacity .2s ease-in-out;
  -webkit-transition: opacity .2s ease-in-out;
}

span.highlight {
  background-color: #ffffd0;
}

</style><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yanzhe Zhang</title>
  
  <meta name="author" content="Yanzhe Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/gatech.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>张彦哲 (Yanzhe Zhang)</name>
              </p>
              <p> I am a second-year computer science Ph.D. student at <a href="https://www.ic.gatech.edu/">Georgia Tech</a>, working with <a href="http://www.diyiyang.com/">Diyi Yang</a>.
                Before coming to Georgia Tech, I received my bachelor's degree from <a href="http://www.en.cs.zju.edu.cn/">Zhejiang University</a> in 2021.
              </p>
              <p> I work as a research intern at Adobe Research (Summer 2022, Summer 2023) with <a href="https://zhangry868.github.io/">Ruiyi Zhang</a>.
              </p>
              <p> I am visiting Stanford University now.
              </p>
              <p style="text-align:center">
                <a href="mailto:z_yanzhe@gatech.edu">Email</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=iJImxvUAAAAJ&amp;hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://github.com/StevenyzZhang/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
            <img style="width:100%;max-width:100%" alt="profile photo" src="rick.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in <strong> natural language processing</strong>  and <strong> artificial intelligence</strong>, especially in the following directions:
              </p>
              <p>
                (1) To make NLP models capable of continually learning multiple tasks and transferring knowledge.
              </p>
              <p>
                (2) To make NLP models more robust and interpretable.
              </p>
              <p>
                (3) To enable NLP models to benefit from and for other modalities and humans.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
            <p>
              (* refers to equal contribution)
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img width="160" src="rick.jpg" alt="DyLAN" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2310.02170">
              <papertitle>Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Evaluation</papertitle>
            </a>
            <br>
            <a href="https://www.semanticscholar.org/author/Zijun-Liu/2117942065">Zijun Liu</a>,
            <strong> Yanzhe Zhang </strong>, 
	          <a href="http://www.lpeng.net/">Peng Li</a>,
            <a href="http://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>,
            <a href="http://www.diyiyang.com/">Diyi Yang</a>
            <br>
            <em>Preprint</em>, 2023
            <br>
            <a href="https://github.com/SALT-NLP/DyLAN">code</a> / <a href="data/dylan.bib">bibtex</a>
            <p>A dynamic framework for multi-LLM-agent collaboration with auto agent evaluation.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img width="160" src="rick.jpg" alt="llavar" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2306.17107">
              <papertitle>LLaVAR: Enhanced Visual Instruction Tuning for Text-rich Image Understanding</papertitle>
            </a>
            <br>
            <strong> Yanzhe Zhang </strong>, 
	          <a href="https://zhangry868.github.io/">Ruiyi Zhang</a>,
            <a href="https://gujiuxiang.com/">Jiuxiang Gu</a>,
            <a href="https://yufanzhou.com/">Yufan Zhou</a>,
            <a href="https://research.adobe.com/person/nedim-lipka/">Nedim Lipka</a>,
            <a href="http://www.diyiyang.com/">Diyi Yang</a>,
            <a href="https://research.adobe.com/person/tong-sun/">Tong Sun</a>
            <br>
            <em>Preprint</em>, 2023
            <br>
            <a href="https://llavar.github.io/">website</a> / <a href="https://github.com/SALT-NLP/LLaVAR">code</a> / <a href="https://huggingface.co/datasets/SALT-NLP/LLaVAR">data</a> / <a href="data/llavar.bib">bibtex</a>
            <p>A multimodal (vision-language, to be honest) large language model that can read text.</p>
          </td>
        </tr>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img width="160" src="rick.jpg" alt="GPD" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2302.03675">
              <papertitle>Auditing Gender Presentation Differences in Text-to-Image Models</papertitle>
            </a>
            <br>
            <strong> Yanzhe Zhang </strong>, 
	          <a href="http://www.lujiang.info/index.html">Lu Jiang</a>,
            <a href="https://faculty.cc.gatech.edu/~turk/">Greg Turk</a>,
            <a href="http://www.diyiyang.com/">Diyi Yang</a>
            <br>
            <em>Preprint</em>, 2023
            <br>
            <a href="https://salt-nlp.github.io/GEP/">website</a> / <a href="https://github.com/SALT-NLP/GEP_data">code</a> / <a href="https://github.com/SALT-NLP/GEP_data">data</a> / <a href="data/GEP.bib">bibtex</a>
            <p>A metric to evaluate attribute-wise differences between genders in text-to-image models.</p>
          </td>
        </tr>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img width="160" src="rick.jpg" alt="LLM" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2302.09185">
              <papertitle>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</papertitle>
            </a>
            <br>
            <a>Albert Lu*</a>
            <a href="https://icefoxzhx.github.io/">Hongxin Zhang*</a>,
            <strong>Yanzhe Zhang</strong>,
            <a href="https://scholar.google.com/citations?user=ScLUQ-YAAAAJ&amp;hl=en">Xuezhi Wang</a>,
            <a href="http://www.diyiyang.com/">Diyi Yang</a>
            <br>
            <em>EACL (Findings)</em>, 2023
            <br>
            <a href="https://github.com/SALT-NLP/Bound-Cap-LLM">code</a> / <a href="data/eacl2023.bib">bibtex</a>
            <p>A study on the limitation of LLM on structural and stylistic constraints.</p>
          </td>
        </tr>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img width="160" src="rick.jpg" alt="RD" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2210.10693">
              <papertitle>Robustness of Demonstration-based Learning Under Limited Data Scenario</papertitle>
            </a>
            <br>
            <a href="https://icefoxzhx.github.io/">Hongxin Zhang</a>,
            <strong> Yanzhe Zhang </strong>, 
            <a href="https://zhangry868.github.io/">Ruiyi Zhang</a>,
            <a href="http://www.diyiyang.com/">Diyi Yang</a>
            <br>
            <em>EMNLP</em>, 2022 <em>(Oral Presentation)</em>
            <br>
            <a href="https://github.com/SALT-NLP/RobustDemo">code</a> / <a href="data/RobustDemo.bib">bibtex</a>
            <p>Astonishingly find that random tokens strings work well as demonstrations.</p>
          </td>
        </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img width="160" src="rick.jpg" alt="ACM" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2203.10652">
                <papertitle>Continual Sequence Generation with Adaptive Compositional Modules</papertitle>
              </a>
              <br>
              <strong> Yanzhe Zhang </strong>, 
	            <a href="https://scholar.google.com/citations?user=ScLUQ-YAAAAJ&amp;hl=en">Xuezhi Wang</a>,
              <a href="http://www.diyiyang.com/">Diyi Yang</a>
              <br>
              <em>ACL</em>, 2022 <em>(Oral Presentation)</em>
              <br>
              <a href="https://github.com/SALT-NLP/Adaptive-Compositional-Modules">code</a> / <a href="data/CL_GEN.bib">bibtex</a>
              <p>Add and reuse adapters strategically in continual sequence generation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img width="160" src="rick.jpg" alt="AdvNER" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2203.10693">
                <papertitle>Leveraging Expert Guided Adversarial Augmentation For Improving Generalization in Named Entity Recognition</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/aaron-reich-10861610b/">Aaron Reich</a>,
              <a href="https://www.jiaaochen.com/">Jiaao Chen</a>,
              <a href="https://www.linkedin.com/in/aastha-agrawal/">Aastha Agrawal</a>,
	            <strong> Yanzhe Zhang </strong>, 
              <a href="http://www.diyiyang.com/">Diyi Yang</a>
              <br>
              <em>ACL (Findings)</em>, 2022
              <br>
              <a href="https://github.com/SALT-NLP/Guided-Adversarial-Augmentation">code</a> / <a href="https://github.com/SALT-NLP/Guided-Adversarial-Augmentation">data</a> / <a href="data/AdvNER.bib">bibtex</a>
              <p>Alter entity type by predefined change on tokens and contexts to attack NER models.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img width="160" src="rick.jpg" alt="IDBR" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://aclanthology.org/2021.naacl-main.218/">
                <papertitle>Continual Learning for Text Classification with Information Disentanglement Based Regularization</papertitle>
              </a>
              <br>
              <a href="https://codeforces.com/profile/yfhuang">Yufan Huang</a>*,
              <strong> Yanzhe Zhang* </strong>, 
              <a href="https://www.jiaaochen.com/">Jiaao Chen</a>,
	            <a href="https://scholar.google.com/citations?user=ScLUQ-YAAAAJ&amp;hl=en">Xuezhi Wang</a>,
              <a href="http://www.diyiyang.com/">Diyi Yang</a>
              <br>
              <em>NAACL</em>, 2021
              <br>
              <a href="https://github.com/SALT-NLP/IDBR">code</a> / <a href="data/NAACL2021.bib">bibtex</a>
              <p>Augment regularization  in continual text classification with two simple auxiliary tasks.</p>
            </td>
          </tr>

        </tbody></table>

				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Service</heading>
            <p>
              <strong>Volunteer:</strong> NAACL 2021.
            </p>
            <p>
              <strong>Reviewer:</strong> EMNLP 2022, ICLR 2023, EACL 2023, ACL 2023, EMNLP 2023.
            </p>
          </td>
        </tr>
      </tbody></table>
        
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website's code is from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </tbody></table>



</body></html>