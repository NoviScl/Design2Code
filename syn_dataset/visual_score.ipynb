{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/Cellar/tesseract/5.3.3/bin/tesseract'\n",
    "\n",
    "def mask_text_cv2(cv2_image):\n",
    "    # Convert the cv2 image (BGR) to PIL Image (RGB)\n",
    "    rgb_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_image)\n",
    "\n",
    "    # Use pytesseract to do OCR on the image\n",
    "    text_data = pytesseract.image_to_data(pil_image)\n",
    "\n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    print(text_data.split('\\n')[0])\n",
    "\n",
    "    # Process the OCR data\n",
    "    for line in text_data.split('\\n')[1:]:\n",
    "        if line.strip() == '':\n",
    "            continue\n",
    "\n",
    "        parts = line.split()\n",
    "        print(parts)\n",
    "        if len(parts) >= 12:\n",
    "            x, y, width, height = map(int, parts[6:10])\n",
    "            # Draw a white rectangle over the detected text\n",
    "            draw.rectangle([x, y, x + width, y + height], fill=\"white\")\n",
    "\n",
    "    # Convert PIL Image back to cv2 format (BGR)\n",
    "    masked_cv2_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "    return masked_cv2_image\n",
    "\n",
    "\n",
    "def check_match_images(src_img, web_img, visualize=False):\n",
    "    # Read the images\n",
    "    image_b = cv2.imread(web_img)\n",
    "    image_b = mask_text_cv2(image_b)\n",
    "    image_a = cv2.imread(src_img)\n",
    "\n",
    "    # SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Find keypoints and descriptors\n",
    "    keypoints_a, descriptors_a = sift.detectAndCompute(image_a, None)\n",
    "    keypoints_b, descriptors_b = sift.detectAndCompute(image_b, None)\n",
    "\n",
    "    # FLANN based matcher\n",
    "    index_params = dict(algorithm=1, trees=5)\n",
    "    search_params = dict()\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = flann.knnMatch(descriptors_a, descriptors_b, k=2)\n",
    "\n",
    "    # Keep good matches: Lowe's ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    if len(good_matches) > 10: # adjust this threshold\n",
    "\n",
    "        image_matches = cv2.drawMatches(image_a, keypoints_a, image_b, keypoints_b, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    \n",
    "        src_pts = np.float32([keypoints_a[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([keypoints_b[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Find homography\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        # Use the homography matrix M to transform the corners of Image A to Image B's plane\n",
    "        h, w = image_a.shape[:2]\n",
    "        pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "        dst = cv2.perspectiveTransform(pts, M)\n",
    "\n",
    "        # Draw the transformed image on Image B\n",
    "        image_b_with_a = cv2.polylines(image_b, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "        # gray = cv2.cvtColor(image_b_with_a, cv2.COLOR_BGR2GRAY)\n",
    "        if visualize:\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            ax.axis('off')\n",
    "            plt.imshow(image_matches)\n",
    "            plt.show()\n",
    "\n",
    "        hb, wb = image_b.shape[:2]\n",
    "        print(hb, wb)\n",
    "        print(dst)\n",
    "\n",
    "        # Extract scale and translation (approximate)\n",
    "        scale_x = np.linalg.norm(dst[1] - dst[0]) / hb\n",
    "        scale_y = np.linalg.norm(dst[2] - dst[1]) / wb\n",
    "        translation = dst[0][0] / np.array([hb, wb])\n",
    "\n",
    "        print(f\"Relative height: {scale_x}, Relative width: {scale_y}\")\n",
    "        print(f\"Top-Left Corner Coordinate: {translation}\")\n",
    "        return scale_x, scale_y, translation.tolist()\n",
    "    else:\n",
    "        print(\"Image not found!\")\n",
    "        return None, None, [None, None]\n",
    "    \n",
    "\n",
    "# check_match_images('../trial_dataset/rick.jpg', './diyi.png')\n",
    "check_match_images('../trial_dataset/rick.jpg', './diyi_gpt4.png', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./diyi_gpt4{color}.png\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"./diyi_gpt4.png\"\n",
    "template = file_path[:-4] + \"{color}\" + file_path[-4:]\n",
    "\n",
    "image = Image.open(template.format(color=\"\")).convert(\"RGB\")\n",
    "image_array = np.array(image)\n",
    "\n",
    "image_red = Image.open(template.format(color=\"_red\")).convert(\"RGB\")\n",
    "image_array_red = np.array(image_red)\n",
    "\n",
    "image_blue = Image.open(template.format(color=\"_blue\")).convert(\"RGB\")\n",
    "image_array_blue = np.array(image_blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_image = (image_array_red[:, :, 0] >= 250) & (image_array_red[:, :, 1] <= 5) & (image_array_red[:, :, 2] <= 5) & (image_array_blue[:, :, 0] <= 5) & (image_array_blue[:, :, 1] <= 5) & (image_array_blue[:, :, 2] >= 250)\n",
    "is_image_coordinates = np.column_stack(np.where(is_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 119 1160 1259\n"
     ]
    }
   ],
   "source": [
    "print(np.min(is_image_coordinates[:, 0]), np.max(is_image_coordinates[:, 0]), np.min(is_image_coordinates[:, 1]), np.max(is_image_coordinates[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from paddleocr import PaddleOCR, draw_ocr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang=\"en\", show_log = False)\n",
    "\n",
    "\n",
    "def get_ocr_blocks(image_path):\n",
    "    # This function will use OCR to extract text blocks and their bounding boxes from an image\n",
    "    image = cv2.imread(image_path)\n",
    "    img_h, img_w, _ = image.shape\n",
    "    data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "    blocks = []\n",
    "    for i in range(len(data['text'])):\n",
    "        if int(data['conf'][i]) > 50:  # Consider blocks with confidence > 60%\n",
    "            (x, y, w, h) = (data['left'][i], data['top'][i], data['width'][i], data['height'][i])\n",
    "            text = data['text'][i].strip()\n",
    "            blocks.append({'text': text, 'bbox': (x / img_w, y / img_h, w / img_w, h / img_h)})\n",
    "    return blocks\n",
    "\n",
    "def is_duplicate(new_block, existing_blocks, threshold=0.1):\n",
    "    new_x, new_y, new_w, new_h = new_block['bbox']\n",
    "    for block in existing_blocks:\n",
    "        existing_x, existing_y, existing_w, existing_h = block['bbox']\n",
    "        \n",
    "        # Check if the new block is close to an existing block in terms of position and size\n",
    "        if (abs(new_x - existing_x) < threshold and abs(new_y - existing_y) < threshold and\n",
    "            abs(new_w - existing_w) < threshold and abs(new_h - existing_h) < threshold):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_ppocr_blocks(image_path, segment_height=1280, overlap=128):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    img_h, img_w, _ = image.shape\n",
    "    \n",
    "    # Initialize an empty list to store all OCR results\n",
    "    all_blocks = []\n",
    "\n",
    "    current_start_y = 0\n",
    "    while current_start_y + overlap < img_h:\n",
    "        # Calculate segment boundaries\n",
    "        start_y = current_start_y\n",
    "        end_y = min(current_start_y + segment_height, img_h)\n",
    "\n",
    "        # Crop the image\n",
    "        cropped_image = image[start_y:end_y, :, :]\n",
    "\n",
    "        # Save the cropped image temporarily\n",
    "        temp_path = f\"temp_cropped_{start_y}.jpg\"\n",
    "        cv2.imwrite(temp_path, cropped_image)\n",
    "\n",
    "        # Perform OCR on the cropped image\n",
    "        result = ocr.ocr(temp_path, cls=True)[0]\n",
    "        if result is None:\n",
    "            current_start_y = end_y - overlap\n",
    "            os.remove(temp_path)\n",
    "            continue\n",
    "\n",
    "        # Delete the temporary file\n",
    "        os.remove(temp_path)\n",
    "\n",
    "        # Process OCR results and adjust coordinates\n",
    "        for i in range(len(result)):\n",
    "            x, y, w, h = result[i][0][0][0], result[i][0][0][1], result[i][0][2][0] - result[i][0][0][0], result[i][0][2][1] - result[i][0][0][1]\n",
    "            text = result[i][1][0].strip()\n",
    "            adjusted_bbox = (x / img_w, (y + start_y) / img_h, w / img_w, h / img_h)\n",
    "            new_block = {'text': text, 'bbox': adjusted_bbox}\n",
    "            \n",
    "            # Check if the new block is a duplicate\n",
    "            if not is_duplicate(new_block, all_blocks):\n",
    "                all_blocks.append(new_block)\n",
    "\n",
    "        # Update the start_y for the next crop\n",
    "        current_start_y = end_y - overlap\n",
    "\n",
    "        print(current_start_y)\n",
    "        print(all_blocks)\n",
    "\n",
    "    return all_blocks\n",
    "\n",
    "def match_blocks(blocks1, blocks2, v_scale=0.1):\n",
    "    # This function will match blocks between two sets based on text similarity, spatial location, and size similarity\n",
    "    matched_blocks = []\n",
    "    max_distance = (1 + v_scale**2)**0.5\n",
    "    \n",
    "    for block1 in blocks1:\n",
    "        best_match = None\n",
    "        highest_score = 0\n",
    "        \n",
    "        for block2 in blocks2:\n",
    "            # Text similarity\n",
    "            text_similarity = SequenceMatcher(None, block1['text'], block2['text']).ratio()\n",
    "            \n",
    "            if text_similarity > 0.8:  # Text must be similar above a threshold\n",
    "                \n",
    "                # Spatial proximity (normalized by image dimensions for example)\n",
    "                spatial_proximity = 1 - ((block1['bbox'][0] - block2['bbox'][0])**2 + (block1['bbox'][1] * v_scale - block2['bbox'][1] * v_scale)**2)**0.5 / max_distance\n",
    "                \n",
    "                # Size similarity\n",
    "                # size_similarity = 1 - abs(block1['bbox'][2]*block1['bbox'][3] - block2['bbox'][2]*block2['bbox'][3]) / max(block1['bbox'][2]*block1['bbox'][3], block2['bbox'][2]*block2['bbox'][3])\n",
    "\n",
    "                # Combine the scores with weights as needed\n",
    "                # combined_score = (text_similarity * 0.6) + (spatial_proximity * 0.2) + (size_similarity * 0.2)\n",
    "                combined_score = (text_similarity * 0.6) + (spatial_proximity * 0.4)\n",
    "\n",
    "                print(block2)\n",
    "                print(combined_score)\n",
    "\n",
    "                if combined_score > highest_score:\n",
    "                    highest_score = combined_score\n",
    "                    best_match = block2\n",
    "\n",
    "        if best_match:\n",
    "            matched_blocks.append((block1, best_match))\n",
    "        \n",
    "        break\n",
    "    \n",
    "    return matched_blocks\n",
    "\n",
    "\n",
    "def calculate_positional_score(bbox1, bbox2, v_scale=0.1):\n",
    "    max_distance = (1 + v_scale**2)**0.5\n",
    "\n",
    "    # Calculate the Euclidean distance between the center points of two bounding boxes\n",
    "    center1 = (bbox1[0] + bbox1[2] / 2, bbox1[1] + bbox1[3] / 2)\n",
    "    center2 = (bbox2[0] + bbox2[2] / 2, bbox2[1] + bbox2[3] / 2)\n",
    "    distance = ((center1[0] - center2[0]) ** 2 + (center1[1] * v_scale - center2[1] * v_scale) ** 2) ** 0.5\n",
    "    \n",
    "    # Normalize distance based on a predefined max distance, this value could be tuned\n",
    "    normalized_distance = min(distance / max_distance, 1)\n",
    "    \n",
    "    # Calculate score using exponential decay\n",
    "    score = 1 - normalized_distance\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n",
      "[{'text': 'Diyi Yang', 'bbox': (0.0140625, 0.0763888888888889, 0.08671875, 0.043055555555555555)}, {'text': 'diyiy@stanford.edu', 'bbox': (0.01484375, 0.18888888888888888, 0.109375, 0.02361111111111111)}, {'text': 'Stanford University', 'bbox': (0.01328125, 0.33055555555555555, 0.1078125, 0.027777777777777776)}, {'text': 'Home', 'bbox': (0.01328125, 0.43333333333333335, 0.03046875, 0.020833333333333332)}, {'text': 'Teaching', 'bbox': (0.16796875, 0.43194444444444446, 0.0421875, 0.025)}, {'text': 'Rehearsal: Simulating Chat to Facilitate Conflict Resolution', 'bbox': (0.01484375, 0.5458333333333333, 0.2453125, 0.018055555555555554)}, {'text': 'Using Large Language Models in Psychology', 'bbox': (0.503125, 0.5680555555555555, 0.19140625, 0.027777777777777776)}, {'text': 'Publications', 'bbox': (0.0140625, 0.6125, 0.0859375, 0.029166666666666667)}]\n",
      "1152\n",
      "[{'text': 'Diyi Yang', 'bbox': (0.61171875, 0.006784771956275914, 0.1046875, 0.007161703731624576)}, {'text': 'Home', 'bbox': (0.21640625, 0.04824726724462872, 0.0390625, 0.003769317753486619)}, {'text': 'Group', 'bbox': (0.39375, 0.04787033546928006, 0.0375, 0.0041462495288352805)}, {'text': 'Teaching', 'bbox': (0.73515625, 0.04711647191858274, 0.05546875, 0.0054655107425555976)}, {'text': 'Recent Preprints', 'bbox': (0.146875, 0.05898982284206559, 0.140625, 0.004523181304183943)}, {'text': 'Rehearsal: Simulating Conflict to Teach Conflict Resolution', 'bbox': (0.16640625, 0.06765925367508481, 0.34296875, 0.003769317753486619)}, {'text': 'Omar Shaikh,Valentino Chai, Michele J.Gelfand, Diyi Yang,Michael S.Bernstein', 'bbox': (0.17890625, 0.07237090086694309, 0.44609375, 0.003580851865812288)}, {'text': 'Shang-Ling Hsu, Raj Sanjay Shah,Prathik Senthil, Zahra Ashktorab, Casey Dugan,Werner Geyer,Diyi Yang', 'bbox': (0.17890625, 0.10346777233320768, 0.59921875, 0.003392385978137957)}, {'text': 'Nature Reviews Psychology,2023. [pdf]', 'bbox': (0.178125, 0.1741424802110818, 0.22734375, 0.003580851865812288)}, {'text': 'Unlearn What You Want to Forget: Efficient Unlearning for LLMs', 'bbox': (0.16640625, 0.18017338861666038, 0.37421875, 0.003769317753486619)}, {'text': 'Jiaao Chen, Diyi Yang', 'bbox': (0.1765625, 0.183942706370147, 0.1234375, 0.005088578967206936)}, {'text': 'CoAnnotating:Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation', 'bbox': (0.16796875, 0.22691292875989447, 0.6734375, 0.003203920090463626)}, {'text': 'Ella Li, Taiwei Shi Caleb ZiemsMin-Yen KanNancy F.Chen,Zhengyuan Liu,Diyi Yang', 'bbox': (0.178125, 0.23124764417640406, 0.484375, 0.003769317753486619)}]\n",
      "2304\n",
      "[{'text': 'Diyi Yang', 'bbox': (0.61171875, 0.006784771956275914, 0.1046875, 0.007161703731624576)}, {'text': 'Home', 'bbox': (0.21640625, 0.04824726724462872, 0.0390625, 0.003769317753486619)}, {'text': 'Group', 'bbox': (0.39375, 0.04787033546928006, 0.0375, 0.0041462495288352805)}, {'text': 'Teaching', 'bbox': (0.73515625, 0.04711647191858274, 0.05546875, 0.0054655107425555976)}, {'text': 'Recent Preprints', 'bbox': (0.146875, 0.05898982284206559, 0.140625, 0.004523181304183943)}, {'text': 'Rehearsal: Simulating Conflict to Teach Conflict Resolution', 'bbox': (0.16640625, 0.06765925367508481, 0.34296875, 0.003769317753486619)}, {'text': 'Omar Shaikh,Valentino Chai, Michele J.Gelfand, Diyi Yang,Michael S.Bernstein', 'bbox': (0.17890625, 0.07237090086694309, 0.44609375, 0.003580851865812288)}, {'text': 'Shang-Ling Hsu, Raj Sanjay Shah,Prathik Senthil, Zahra Ashktorab, Casey Dugan,Werner Geyer,Diyi Yang', 'bbox': (0.17890625, 0.10346777233320768, 0.59921875, 0.003392385978137957)}, {'text': 'Nature Reviews Psychology,2023. [pdf]', 'bbox': (0.178125, 0.1741424802110818, 0.22734375, 0.003580851865812288)}, {'text': 'Unlearn What You Want to Forget: Efficient Unlearning for LLMs', 'bbox': (0.16640625, 0.18017338861666038, 0.37421875, 0.003769317753486619)}, {'text': 'Jiaao Chen, Diyi Yang', 'bbox': (0.1765625, 0.183942706370147, 0.1234375, 0.005088578967206936)}, {'text': 'CoAnnotating:Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation', 'bbox': (0.16796875, 0.22691292875989447, 0.6734375, 0.003203920090463626)}, {'text': 'Ella Li, Taiwei Shi Caleb ZiemsMin-Yen KanNancy F.Chen,Zhengyuan Liu,Diyi Yang', 'bbox': (0.178125, 0.23124764417640406, 0.484375, 0.003769317753486619)}, {'text': 'ACL,2023.[pdf][website]', 'bbox': (0.178125, 0.28646814926498304, 0.14765625, 0.003769317753486619)}, {'text': 'NormBank:A Knowledge Bank of Situational Social Norms', 'bbox': (0.16796875, 0.29325292122125896, 0.34140625, 0.003203920090463626)}, {'text': 'Omar ShaikhCaleb Ziems, William Held,Aryan J.Pariani,Fred Morstatter, Diyi Yang', 'bbox': (0.178125, 0.3791933660007539, 0.471875, 0.003769317753486619)}, {'text': 'CSCW2023.[pdf]', 'bbox': (0.178125, 0.3987938183188843, 0.109375, 0.004334715416509612)}, {'text': 'Parameter-Efficient Fine-Tuning Design Spaces', 'bbox': (0.16640625, 0.4055785902751602, 0.27734375, 0.003769317753486619)}]\n",
      "3456\n",
      "[{'text': 'Diyi Yang', 'bbox': (0.61171875, 0.006784771956275914, 0.1046875, 0.007161703731624576)}, {'text': 'Home', 'bbox': (0.21640625, 0.04824726724462872, 0.0390625, 0.003769317753486619)}, {'text': 'Group', 'bbox': (0.39375, 0.04787033546928006, 0.0375, 0.0041462495288352805)}, {'text': 'Teaching', 'bbox': (0.73515625, 0.04711647191858274, 0.05546875, 0.0054655107425555976)}, {'text': 'Recent Preprints', 'bbox': (0.146875, 0.05898982284206559, 0.140625, 0.004523181304183943)}, {'text': 'Rehearsal: Simulating Conflict to Teach Conflict Resolution', 'bbox': (0.16640625, 0.06765925367508481, 0.34296875, 0.003769317753486619)}, {'text': 'Omar Shaikh,Valentino Chai, Michele J.Gelfand, Diyi Yang,Michael S.Bernstein', 'bbox': (0.17890625, 0.07237090086694309, 0.44609375, 0.003580851865812288)}, {'text': 'Shang-Ling Hsu, Raj Sanjay Shah,Prathik Senthil, Zahra Ashktorab, Casey Dugan,Werner Geyer,Diyi Yang', 'bbox': (0.17890625, 0.10346777233320768, 0.59921875, 0.003392385978137957)}, {'text': 'Nature Reviews Psychology,2023. [pdf]', 'bbox': (0.178125, 0.1741424802110818, 0.22734375, 0.003580851865812288)}, {'text': 'Unlearn What You Want to Forget: Efficient Unlearning for LLMs', 'bbox': (0.16640625, 0.18017338861666038, 0.37421875, 0.003769317753486619)}, {'text': 'Jiaao Chen, Diyi Yang', 'bbox': (0.1765625, 0.183942706370147, 0.1234375, 0.005088578967206936)}, {'text': 'CoAnnotating:Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation', 'bbox': (0.16796875, 0.22691292875989447, 0.6734375, 0.003203920090463626)}, {'text': 'Ella Li, Taiwei Shi Caleb ZiemsMin-Yen KanNancy F.Chen,Zhengyuan Liu,Diyi Yang', 'bbox': (0.178125, 0.23124764417640406, 0.484375, 0.003769317753486619)}, {'text': 'ACL,2023.[pdf][website]', 'bbox': (0.178125, 0.28646814926498304, 0.14765625, 0.003769317753486619)}, {'text': 'NormBank:A Knowledge Bank of Situational Social Norms', 'bbox': (0.16796875, 0.29325292122125896, 0.34140625, 0.003203920090463626)}, {'text': 'Omar ShaikhCaleb Ziems, William Held,Aryan J.Pariani,Fred Morstatter, Diyi Yang', 'bbox': (0.178125, 0.3791933660007539, 0.471875, 0.003769317753486619)}, {'text': 'CSCW2023.[pdf]', 'bbox': (0.178125, 0.3987938183188843, 0.109375, 0.004334715416509612)}, {'text': 'Parameter-Efficient Fine-Tuning Design Spaces', 'bbox': (0.16640625, 0.4055785902751602, 0.27734375, 0.003769317753486619)}, {'text': 'Raj Sanjay Shah, Faye Holt, Shirley Anugrah Hayati, Aastha Agrawal, Yi-Chia WangRobert Kraut, Diyi Yang', 'bbox': (0.178125, 0.472107048624199, 0.60625, 0.003580851865812288)}, {'text': 'Camille Harris, Matan HalevyAyanna Howard,Amy Bruckman,Diyi Yang', 'bbox': (0.178125, 0.502826988315115, 0.4125, 0.00395778364116095)}, {'text': 'FAccT,2022.[pdf]', 'bbox': (0.17734375, 0.5073501696192989, 0.10625, 0.003769317753486619)}, {'text': 'VALUE: Understanding Dialect Disparity in NLU', 'bbox': (0.16796875, 0.5141349415755748, 0.27890625, 0.003203920090463626)}, {'text': 'Will Al Console Me when l Lose my Pet? Understanding Perceptions of Al-Mediated Email Writing', 'bbox': (0.16640625, 0.5799095363739163, 0.5625, 0.0041462495288352805)}, {'text': 'Best Paper Honorable Mention', 'bbox': (0.17890625, 0.6093102148511119, 0.175, 0.003392385978137957)}, {'text': 'Latent Hatred:A Benchmark for Understanding Implicit Hate Speech', 'bbox': (0.16640625, 0.63098379193366, 0.396875, 0.003769317753486619)}, {'text': 'Mai ElSherief*.Caleb Ziems*,David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury,Diyi Yang', 'bbox': (0.17734375, 0.635506973237844, 0.66328125, 0.003769317753486619)}]\n",
      "4608\n",
      "[{'text': 'Diyi Yang', 'bbox': (0.61171875, 0.006784771956275914, 0.1046875, 0.007161703731624576)}, {'text': 'Home', 'bbox': (0.21640625, 0.04824726724462872, 0.0390625, 0.003769317753486619)}, {'text': 'Group', 'bbox': (0.39375, 0.04787033546928006, 0.0375, 0.0041462495288352805)}, {'text': 'Teaching', 'bbox': (0.73515625, 0.04711647191858274, 0.05546875, 0.0054655107425555976)}, {'text': 'Recent Preprints', 'bbox': (0.146875, 0.05898982284206559, 0.140625, 0.004523181304183943)}, {'text': 'Rehearsal: Simulating Conflict to Teach Conflict Resolution', 'bbox': (0.16640625, 0.06765925367508481, 0.34296875, 0.003769317753486619)}, {'text': 'Omar Shaikh,Valentino Chai, Michele J.Gelfand, Diyi Yang,Michael S.Bernstein', 'bbox': (0.17890625, 0.07237090086694309, 0.44609375, 0.003580851865812288)}, {'text': 'Shang-Ling Hsu, Raj Sanjay Shah,Prathik Senthil, Zahra Ashktorab, Casey Dugan,Werner Geyer,Diyi Yang', 'bbox': (0.17890625, 0.10346777233320768, 0.59921875, 0.003392385978137957)}, {'text': 'Nature Reviews Psychology,2023. [pdf]', 'bbox': (0.178125, 0.1741424802110818, 0.22734375, 0.003580851865812288)}, {'text': 'Unlearn What You Want to Forget: Efficient Unlearning for LLMs', 'bbox': (0.16640625, 0.18017338861666038, 0.37421875, 0.003769317753486619)}, {'text': 'Jiaao Chen, Diyi Yang', 'bbox': (0.1765625, 0.183942706370147, 0.1234375, 0.005088578967206936)}, {'text': 'CoAnnotating:Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation', 'bbox': (0.16796875, 0.22691292875989447, 0.6734375, 0.003203920090463626)}, {'text': 'Ella Li, Taiwei Shi Caleb ZiemsMin-Yen KanNancy F.Chen,Zhengyuan Liu,Diyi Yang', 'bbox': (0.178125, 0.23124764417640406, 0.484375, 0.003769317753486619)}, {'text': 'ACL,2023.[pdf][website]', 'bbox': (0.178125, 0.28646814926498304, 0.14765625, 0.003769317753486619)}, {'text': 'NormBank:A Knowledge Bank of Situational Social Norms', 'bbox': (0.16796875, 0.29325292122125896, 0.34140625, 0.003203920090463626)}, {'text': 'Omar ShaikhCaleb Ziems, William Held,Aryan J.Pariani,Fred Morstatter, Diyi Yang', 'bbox': (0.178125, 0.3791933660007539, 0.471875, 0.003769317753486619)}, {'text': 'CSCW2023.[pdf]', 'bbox': (0.178125, 0.3987938183188843, 0.109375, 0.004334715416509612)}, {'text': 'Parameter-Efficient Fine-Tuning Design Spaces', 'bbox': (0.16640625, 0.4055785902751602, 0.27734375, 0.003769317753486619)}, {'text': 'Raj Sanjay Shah, Faye Holt, Shirley Anugrah Hayati, Aastha Agrawal, Yi-Chia WangRobert Kraut, Diyi Yang', 'bbox': (0.178125, 0.472107048624199, 0.60625, 0.003580851865812288)}, {'text': 'Camille Harris, Matan HalevyAyanna Howard,Amy Bruckman,Diyi Yang', 'bbox': (0.178125, 0.502826988315115, 0.4125, 0.00395778364116095)}, {'text': 'FAccT,2022.[pdf]', 'bbox': (0.17734375, 0.5073501696192989, 0.10625, 0.003769317753486619)}, {'text': 'VALUE: Understanding Dialect Disparity in NLU', 'bbox': (0.16796875, 0.5141349415755748, 0.27890625, 0.003203920090463626)}, {'text': 'Will Al Console Me when l Lose my Pet? Understanding Perceptions of Al-Mediated Email Writing', 'bbox': (0.16640625, 0.5799095363739163, 0.5625, 0.0041462495288352805)}, {'text': 'Best Paper Honorable Mention', 'bbox': (0.17890625, 0.6093102148511119, 0.175, 0.003392385978137957)}, {'text': 'Latent Hatred:A Benchmark for Understanding Implicit Hate Speech', 'bbox': (0.16640625, 0.63098379193366, 0.396875, 0.003769317753486619)}, {'text': 'Mai ElSherief*.Caleb Ziems*,David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury,Diyi Yang', 'bbox': (0.17734375, 0.635506973237844, 0.66328125, 0.003769317753486619)}, {'text': 'Understanding the Usage of Online Media for Parenting from Infancy to Preschool At Scale', 'bbox': (0.16640625, 0.7018469656992085, 0.52109375, 0.003769317753486619)}, {'text': 'SIGCHI,2021.[pdf]', 'bbox': (0.178125, 0.710704862419902, 0.11328125, 0.00395778364116095)}, {'text': 'Yufan Huang,Yanzhe Zhang,Jiaao ChenXuezhi Wang and Diyi Yang', 'bbox': (0.17734375, 0.7418017338861667, 0.3859375, 0.003769317753486619)}, {'text': 'Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization', 'bbox': (0.16640625, 0.7681869581605729, 0.6515625, 0.003769317753486619)}, {'text': 'Jiaao Chen,Zichao Yang,and Diyi Yang', 'bbox': (0.178125, 0.7883528081417264, 0.21796875, 0.003769317753486619)}, {'text': 'Reid Pryzant,Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky,and Diyi Yang', 'bbox': (0.17734375, 0.818884281944968, 0.54453125, 0.0041462495288352805)}, {'text': 'Diyi YangRobert Kraut, Tenbroeck Smith, Elijah Mayfield, and Dan Jurafsky', 'bbox': (0.178125, 0.8501696192989069, 0.421875, 0.003769317753486619)}, {'text': 'CHI, 2019[pdf]', 'bbox': (0.17890625, 0.8746701846965699, 0.08984375, 0.003580851865812288)}]\n",
      "5178\n",
      "[{'text': 'Diyi Yang', 'bbox': (0.61171875, 0.006784771956275914, 0.1046875, 0.007161703731624576)}, {'text': 'Home', 'bbox': (0.21640625, 0.04824726724462872, 0.0390625, 0.003769317753486619)}, {'text': 'Group', 'bbox': (0.39375, 0.04787033546928006, 0.0375, 0.0041462495288352805)}, {'text': 'Teaching', 'bbox': (0.73515625, 0.04711647191858274, 0.05546875, 0.0054655107425555976)}, {'text': 'Recent Preprints', 'bbox': (0.146875, 0.05898982284206559, 0.140625, 0.004523181304183943)}, {'text': 'Rehearsal: Simulating Conflict to Teach Conflict Resolution', 'bbox': (0.16640625, 0.06765925367508481, 0.34296875, 0.003769317753486619)}, {'text': 'Omar Shaikh,Valentino Chai, Michele J.Gelfand, Diyi Yang,Michael S.Bernstein', 'bbox': (0.17890625, 0.07237090086694309, 0.44609375, 0.003580851865812288)}, {'text': 'Shang-Ling Hsu, Raj Sanjay Shah,Prathik Senthil, Zahra Ashktorab, Casey Dugan,Werner Geyer,Diyi Yang', 'bbox': (0.17890625, 0.10346777233320768, 0.59921875, 0.003392385978137957)}, {'text': 'Nature Reviews Psychology,2023. [pdf]', 'bbox': (0.178125, 0.1741424802110818, 0.22734375, 0.003580851865812288)}, {'text': 'Unlearn What You Want to Forget: Efficient Unlearning for LLMs', 'bbox': (0.16640625, 0.18017338861666038, 0.37421875, 0.003769317753486619)}, {'text': 'Jiaao Chen, Diyi Yang', 'bbox': (0.1765625, 0.183942706370147, 0.1234375, 0.005088578967206936)}, {'text': 'CoAnnotating:Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation', 'bbox': (0.16796875, 0.22691292875989447, 0.6734375, 0.003203920090463626)}, {'text': 'Ella Li, Taiwei Shi Caleb ZiemsMin-Yen KanNancy F.Chen,Zhengyuan Liu,Diyi Yang', 'bbox': (0.178125, 0.23124764417640406, 0.484375, 0.003769317753486619)}, {'text': 'ACL,2023.[pdf][website]', 'bbox': (0.178125, 0.28646814926498304, 0.14765625, 0.003769317753486619)}, {'text': 'NormBank:A Knowledge Bank of Situational Social Norms', 'bbox': (0.16796875, 0.29325292122125896, 0.34140625, 0.003203920090463626)}, {'text': 'Omar ShaikhCaleb Ziems, William Held,Aryan J.Pariani,Fred Morstatter, Diyi Yang', 'bbox': (0.178125, 0.3791933660007539, 0.471875, 0.003769317753486619)}, {'text': 'CSCW2023.[pdf]', 'bbox': (0.178125, 0.3987938183188843, 0.109375, 0.004334715416509612)}, {'text': 'Parameter-Efficient Fine-Tuning Design Spaces', 'bbox': (0.16640625, 0.4055785902751602, 0.27734375, 0.003769317753486619)}, {'text': 'Raj Sanjay Shah, Faye Holt, Shirley Anugrah Hayati, Aastha Agrawal, Yi-Chia WangRobert Kraut, Diyi Yang', 'bbox': (0.178125, 0.472107048624199, 0.60625, 0.003580851865812288)}, {'text': 'Camille Harris, Matan HalevyAyanna Howard,Amy Bruckman,Diyi Yang', 'bbox': (0.178125, 0.502826988315115, 0.4125, 0.00395778364116095)}, {'text': 'FAccT,2022.[pdf]', 'bbox': (0.17734375, 0.5073501696192989, 0.10625, 0.003769317753486619)}, {'text': 'VALUE: Understanding Dialect Disparity in NLU', 'bbox': (0.16796875, 0.5141349415755748, 0.27890625, 0.003203920090463626)}, {'text': 'Will Al Console Me when l Lose my Pet? Understanding Perceptions of Al-Mediated Email Writing', 'bbox': (0.16640625, 0.5799095363739163, 0.5625, 0.0041462495288352805)}, {'text': 'Best Paper Honorable Mention', 'bbox': (0.17890625, 0.6093102148511119, 0.175, 0.003392385978137957)}, {'text': 'Latent Hatred:A Benchmark for Understanding Implicit Hate Speech', 'bbox': (0.16640625, 0.63098379193366, 0.396875, 0.003769317753486619)}, {'text': 'Mai ElSherief*.Caleb Ziems*,David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury,Diyi Yang', 'bbox': (0.17734375, 0.635506973237844, 0.66328125, 0.003769317753486619)}, {'text': 'Understanding the Usage of Online Media for Parenting from Infancy to Preschool At Scale', 'bbox': (0.16640625, 0.7018469656992085, 0.52109375, 0.003769317753486619)}, {'text': 'SIGCHI,2021.[pdf]', 'bbox': (0.178125, 0.710704862419902, 0.11328125, 0.00395778364116095)}, {'text': 'Yufan Huang,Yanzhe Zhang,Jiaao ChenXuezhi Wang and Diyi Yang', 'bbox': (0.17734375, 0.7418017338861667, 0.3859375, 0.003769317753486619)}, {'text': 'Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization', 'bbox': (0.16640625, 0.7681869581605729, 0.6515625, 0.003769317753486619)}, {'text': 'Jiaao Chen,Zichao Yang,and Diyi Yang', 'bbox': (0.178125, 0.7883528081417264, 0.21796875, 0.003769317753486619)}, {'text': 'Reid Pryzant,Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky,and Diyi Yang', 'bbox': (0.17734375, 0.818884281944968, 0.54453125, 0.0041462495288352805)}, {'text': 'Diyi YangRobert Kraut, Tenbroeck Smith, Elijah Mayfield, and Dan Jurafsky', 'bbox': (0.178125, 0.8501696192989069, 0.421875, 0.003769317753486619)}, {'text': 'CHI, 2019[pdf]', 'bbox': (0.17890625, 0.8746701846965699, 0.08984375, 0.003580851865812288)}, {'text': 'Diyi Yang,Robert Kraut and John Levine', 'bbox': (0.178125, 0.9053901243874859, 0.225, 0.00395778364116095)}, {'text': 'Putting Humans in the Natural Language Processing Loop:A Survey', 'bbox': (0.16640625, 0.9662646061062947, 0.3953125, 0.00395778364116095)}]\n"
     ]
    }
   ],
   "source": [
    "blocks1 = get_ppocr_blocks('./diyi_gpt4.png')\n",
    "blocks2 = get_ppocr_blocks('./diyi.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_blocks_by_row(blocks, line_overlap_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Group blocks into rows based on their bounding box y-coordinates.\n",
    "    Blocks that have y-overlapping bounding boxes within a threshold are considered to be on the same row.\n",
    "\n",
    "    :param blocks: List of block dictionaries with 'bbox' as one of the keys.\n",
    "    :param line_overlap_threshold: Threshold for considering blocks to be on the same line (relative to image height).\n",
    "    :return: A list of lists of blocks, with each inner list representing a row.\n",
    "    \"\"\"\n",
    "    # Sort blocks by the top y-coordinate\n",
    "    sorted_blocks = sorted(blocks, key=lambda b: b['bbox'][1])\n",
    "    \n",
    "    rows = []\n",
    "    current_row = []\n",
    "    \n",
    "    for block in sorted_blocks:\n",
    "        # If current_row is empty, start a new row with the current block\n",
    "        if not current_row:\n",
    "            current_row.append(block)\n",
    "        else:\n",
    "            # Compare the current block with the last block in the current row\n",
    "            last_block_in_row = current_row[-1]\n",
    "            # Calculate the vertical overlap between the two blocks\n",
    "            top_y_current = block['bbox'][1]\n",
    "            bottom_y_last = last_block_in_row['bbox'][1] + last_block_in_row['bbox'][3]\n",
    "            vertical_overlap = max(0, bottom_y_last - top_y_current)\n",
    "            \n",
    "            # If there is enough overlap, add the block to the current row\n",
    "            if vertical_overlap > line_overlap_threshold * min(last_block_in_row['bbox'][3], block['bbox'][3]):\n",
    "                current_row.append(block)\n",
    "            else:\n",
    "                # Otherwise, the current block starts a new row\n",
    "                rows.extend(current_row)\n",
    "                current_row = [block]\n",
    "    \n",
    "    # Add the last row if it's not empty\n",
    "    if current_row:\n",
    "        rows.extend(current_row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "grouped_blocks1 = group_blocks_by_row(blocks1)\n",
    "grouped_blocks2 = group_blocks_by_row(blocks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 36\n"
     ]
    }
   ],
   "source": [
    "print(len(grouped_blocks1), len(grouped_blocks2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching pairs:\n",
      "{'text': 'Diyi Yang', 'bbox': (0.0140625, 0.0763888888888889, 0.08671875, 0.043055555555555555)} matched with {'text': 'Diyi Yang', 'bbox': (0.61171875, 0.006784771956275914, 0.1046875, 0.007161703731624576)}\n",
      "{'text': 'diyiy@stanford.edu', 'bbox': (0.01484375, 0.18888888888888888, 0.109375, 0.02361111111111111)} matched with {'text': 'Diyi Yang,Robert Kraut and John Levine', 'bbox': (0.178125, 0.9053901243874859, 0.225, 0.00395778364116095)}\n",
      "{'text': 'Stanford University', 'bbox': (0.01328125, 0.33055555555555555, 0.1078125, 0.027777777777777776)} matched with {'text': 'Best Paper Honorable Mention', 'bbox': (0.17890625, 0.6093102148511119, 0.175, 0.003392385978137957)}\n",
      "{'text': 'Teaching', 'bbox': (0.16796875, 0.43194444444444446, 0.0421875, 0.025)} matched with {'text': 'Teaching', 'bbox': (0.73515625, 0.04711647191858274, 0.05546875, 0.0054655107425555976)}\n",
      "{'text': 'Home', 'bbox': (0.01328125, 0.43333333333333335, 0.03046875, 0.020833333333333332)} matched with {'text': 'Home', 'bbox': (0.21640625, 0.04824726724462872, 0.0390625, 0.003769317753486619)}\n",
      "{'text': 'Rehearsal: Simulating Chat to Facilitate Conflict Resolution', 'bbox': (0.01484375, 0.5458333333333333, 0.2453125, 0.018055555555555554)} matched with {'text': 'Rehearsal: Simulating Conflict to Teach Conflict Resolution', 'bbox': (0.16640625, 0.06765925367508481, 0.34296875, 0.003769317753486619)}\n",
      "{'text': 'Using Large Language Models in Psychology', 'bbox': (0.503125, 0.5680555555555555, 0.19140625, 0.027777777777777776)} matched with {'text': 'Nature Reviews Psychology,2023. [pdf]', 'bbox': (0.178125, 0.1741424802110818, 0.22734375, 0.003580851865812288)}\n",
      "{'text': 'Publications', 'bbox': (0.0140625, 0.6125, 0.0859375, 0.029166666666666667)} matched with {'text': 'Recent Preprints', 'bbox': (0.146875, 0.05898982284206559, 0.140625, 0.004523181304183943)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def calculate_similarity(block1, block2, max_distance=1.42):\n",
    "    text_similarity = SequenceMatcher(None, block1['text'], block2['text']).ratio()\n",
    "    spatial_proximity = 1 - ((block1['bbox'][0] - block2['bbox'][0])**2 + (block1['bbox'][1] - block2['bbox'][1])**2)**0.5 / max_distance\n",
    "    combined_score = (text_similarity * 1.0) + (spatial_proximity * 0.0)\n",
    "    return combined_score\n",
    "\n",
    "def adjust_cost_for_context(cost_matrix, consecutive_bonus=1.0, window_size=20):\n",
    "    n, m = cost_matrix.shape\n",
    "    adjusted_cost_matrix = np.copy(cost_matrix)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            bonus = 0\n",
    "            if adjusted_cost_matrix[i][j] >= -0.5:\n",
    "                continue\n",
    "            # Check left context\n",
    "            for k in range(1, window_size + 1):\n",
    "                if i >= k and j >= k:\n",
    "                    # bonus += consecutive_bonus * (cost_matrix[i-k, j-k] < 0)\n",
    "                    bonus += consecutive_bonus * cost_matrix[i-k, j-k]\n",
    "            # Check right context\n",
    "            for k in range(1, window_size + 1):\n",
    "                if i + k < n and j + k < m:\n",
    "                    # bonus += consecutive_bonus * (cost_matrix[i+k, j+k] < 0\n",
    "                    bonus += consecutive_bonus * cost_matrix[i+k, j+k]\n",
    "            adjusted_cost_matrix[i][j] += bonus\n",
    "    return adjusted_cost_matrix\n",
    "\n",
    "def create_cost_matrix(A, B):\n",
    "    n = len(A)\n",
    "    m = len(B)\n",
    "    cost_matrix = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            cost_matrix[i, j] = -calculate_similarity(A[i], B[j])\n",
    "    return cost_matrix\n",
    "\n",
    "def find_maximum_matching(A, B, consecutive_bonus, window_size):\n",
    "    cost_matrix = create_cost_matrix(A, B)\n",
    "    cost_matrix = adjust_cost_for_context(cost_matrix, consecutive_bonus, window_size)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    return list(zip(row_ind, col_ind))\n",
    "\n",
    "# Example usage\n",
    "A = grouped_blocks1\n",
    "B = grouped_blocks2\n",
    "\n",
    "matching = find_maximum_matching(A, B, 0.1, 1)\n",
    "matched_list = []\n",
    "print(\"Matching pairs:\")\n",
    "for i, j in matching:\n",
    "    print(f\"{A[i]} matched with {B[j]}\")\n",
    "    matched_list.append([A[i]['bbox'], B[j]['bbox']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def draw_matched_bboxes(img1, img2, matched_bboxes):\n",
    "    # Create copies of images to draw on\n",
    "    img1_drawn = img1.copy()\n",
    "    img2_drawn = img2.copy()\n",
    "\n",
    "    h1, w1, _ = img1.shape\n",
    "    h2, w2, _ = img2.shape\n",
    "    \n",
    "\n",
    "    # Iterate over matched bounding boxes\n",
    "    for bbox_pair in matched_bboxes:\n",
    "        # Random color for each pair\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "        # Ensure that the bounding box coordinates are integers\n",
    "        bbox1 = [int(bbox_pair[0][0] * w1), int(bbox_pair[0][1] * h1), int(bbox_pair[0][2] * w1), int(bbox_pair[0][3] * h1)]\n",
    "        bbox2 = [int(bbox_pair[1][0] * w2), int(bbox_pair[1][1] * h2), int(bbox_pair[1][2] * w2), int(bbox_pair[1][3] * h2)]\n",
    "\n",
    "        # Draw bbox on the first image\n",
    "        top_left_1 = (bbox1[0], bbox1[1])\n",
    "        bottom_right_1 = (bbox1[0] + bbox1[2], bbox1[1] + bbox1[3])\n",
    "        img1_drawn = cv2.rectangle(img1_drawn, top_left_1, bottom_right_1, color, 2)\n",
    "\n",
    "        # Draw bbox on the second image\n",
    "        top_left_2 = (bbox2[0], bbox2[1])\n",
    "        bottom_right_2 = (bbox2[0] + bbox2[2], bbox2[1] + bbox2[3])\n",
    "        img2_drawn = cv2.rectangle(img2_drawn, top_left_2, bottom_right_2, color, 2)\n",
    "\n",
    "    return img1_drawn, img2_drawn\n",
    "\n",
    "img1 = cv2.imread('./diyi_gpt4.png')\n",
    "img2 = cv2.imread('./diyi.png')\n",
    "\n",
    "img1_with_boxes, img2_with_boxes = draw_matched_bboxes(img1, img2, matched_list)\n",
    "\n",
    "cv2.imwrite('./diyi_gpt4_demo.png', img1_with_boxes)\n",
    "cv2.imwrite('./diyi_demo.png', img2_with_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from paddleocr import PaddleOCR\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n",
      "[{'text': 'Diyi Yang', 'bbox': (0.01484375, 0.07361111111111111, 0.0984375, 0.044444444444444446)}, {'text': 'diyiy@stanford.edu', 'bbox': (0.015625, 0.18888888888888888, 0.12265625, 0.02638888888888889)}, {'text': 'Natural Language Processing Group', 'bbox': (0.01328125, 0.2847222222222222, 0.22734375, 0.027777777777777776)}, {'text': 'Stanford University', 'bbox': (0.01484375, 0.3347222222222222, 0.11953125, 0.02361111111111111)}, {'text': 'Publications', 'bbox': (0.10859375, 0.4361111111111111, 0.06328125, 0.019444444444444445)}, {'text': 'Rehearsal: Simulating Chat to Facilitate Conflict Resolution', 'bbox': (0.01484375, 0.5444444444444444, 0.27578125, 0.02361111111111111)}, {'text': 'Using Large Language Models in Psychology', 'bbox': (0.50546875, 0.5736111111111111, 0.209375, 0.02361111111111111)}, {'text': 'Publications', 'bbox': (0.015625, 0.6194444444444445, 0.09765625, 0.02638888888888889)}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'Diyi Yang',\n",
       "  'bbox': (0.01484375, 0.07361111111111111, 0.0984375, 0.044444444444444446)},\n",
       " {'text': 'diyiy@stanford.edu',\n",
       "  'bbox': (0.015625, 0.18888888888888888, 0.12265625, 0.02638888888888889)},\n",
       " {'text': 'Natural Language Processing Group',\n",
       "  'bbox': (0.01328125, 0.2847222222222222, 0.22734375, 0.027777777777777776)},\n",
       " {'text': 'Stanford University',\n",
       "  'bbox': (0.01484375, 0.3347222222222222, 0.11953125, 0.02361111111111111)},\n",
       " {'text': 'Publications',\n",
       "  'bbox': (0.10859375, 0.4361111111111111, 0.06328125, 0.019444444444444445)},\n",
       " {'text': 'Rehearsal: Simulating Chat to Facilitate Conflict Resolution',\n",
       "  'bbox': (0.01484375, 0.5444444444444444, 0.27578125, 0.02361111111111111)},\n",
       " {'text': 'Using Large Language Models in Psychology',\n",
       "  'bbox': (0.50546875, 0.5736111111111111, 0.209375, 0.02361111111111111)},\n",
       " {'text': 'Publications',\n",
       "  'bbox': (0.015625, 0.6194444444444445, 0.09765625, 0.02638888888888889)}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "get_ppocr_blocks('./diyi_gpt4.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "reader = easyocr.Reader(['en']) # this needs to run only once to load the model into memory\n",
    "result = reader.readtext('./diyi_gpt4.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
