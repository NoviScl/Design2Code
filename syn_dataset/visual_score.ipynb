{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/Cellar/tesseract/5.3.3/bin/tesseract'\n",
    "\n",
    "def mask_text_cv2(cv2_image):\n",
    "    # Convert the cv2 image (BGR) to PIL Image (RGB)\n",
    "    rgb_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_image)\n",
    "\n",
    "    # Use pytesseract to do OCR on the image\n",
    "    text_data = pytesseract.image_to_data(pil_image)\n",
    "\n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    print(text_data.split('\\n')[0])\n",
    "\n",
    "    # Process the OCR data\n",
    "    for line in text_data.split('\\n')[1:]:\n",
    "        if line.strip() == '':\n",
    "            continue\n",
    "\n",
    "        parts = line.split()\n",
    "        print(parts)\n",
    "        if len(parts) >= 12:\n",
    "            x, y, width, height = map(int, parts[6:10])\n",
    "            # Draw a white rectangle over the detected text\n",
    "            draw.rectangle([x, y, x + width, y + height], fill=\"white\")\n",
    "\n",
    "    # Convert PIL Image back to cv2 format (BGR)\n",
    "    masked_cv2_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "    return masked_cv2_image\n",
    "\n",
    "\n",
    "def check_match_images(src_img, web_img, visualize=False):\n",
    "    # Read the images\n",
    "    image_b = cv2.imread(web_img)\n",
    "    image_b = mask_text_cv2(image_b)\n",
    "    image_a = cv2.imread(src_img)\n",
    "\n",
    "    # SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Find keypoints and descriptors\n",
    "    keypoints_a, descriptors_a = sift.detectAndCompute(image_a, None)\n",
    "    keypoints_b, descriptors_b = sift.detectAndCompute(image_b, None)\n",
    "\n",
    "    # FLANN based matcher\n",
    "    index_params = dict(algorithm=1, trees=5)\n",
    "    search_params = dict()\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = flann.knnMatch(descriptors_a, descriptors_b, k=2)\n",
    "\n",
    "    # Keep good matches: Lowe's ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    if len(good_matches) > 10: # adjust this threshold\n",
    "\n",
    "        image_matches = cv2.drawMatches(image_a, keypoints_a, image_b, keypoints_b, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    \n",
    "        src_pts = np.float32([keypoints_a[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([keypoints_b[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Find homography\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        # Use the homography matrix M to transform the corners of Image A to Image B's plane\n",
    "        h, w = image_a.shape[:2]\n",
    "        pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "        dst = cv2.perspectiveTransform(pts, M)\n",
    "\n",
    "        # Draw the transformed image on Image B\n",
    "        image_b_with_a = cv2.polylines(image_b, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "        # gray = cv2.cvtColor(image_b_with_a, cv2.COLOR_BGR2GRAY)\n",
    "        if visualize:\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            ax.axis('off')\n",
    "            plt.imshow(image_matches)\n",
    "            plt.show()\n",
    "\n",
    "        hb, wb = image_b.shape[:2]\n",
    "        print(hb, wb)\n",
    "        print(dst)\n",
    "\n",
    "        # Extract scale and translation (approximate)\n",
    "        scale_x = np.linalg.norm(dst[1] - dst[0]) / hb\n",
    "        scale_y = np.linalg.norm(dst[2] - dst[1]) / wb\n",
    "        translation = dst[0][0] / np.array([hb, wb])\n",
    "\n",
    "        print(f\"Relative height: {scale_x}, Relative width: {scale_y}\")\n",
    "        print(f\"Top-Left Corner Coordinate: {translation}\")\n",
    "        return scale_x, scale_y, translation.tolist()\n",
    "    else:\n",
    "        print(\"Image not found!\")\n",
    "        return None, None, [None, None]\n",
    "    \n",
    "\n",
    "# check_match_images('../trial_dataset/rick.jpg', './diyi.png')\n",
    "check_match_images('../trial_dataset/rick.jpg', './diyi_gpt4.png', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./diyi_gpt4{color}.png\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"./diyi_gpt4.png\"\n",
    "template = file_path[:-4] + \"{color}\" + file_path[-4:]\n",
    "\n",
    "image = Image.open(template.format(color=\"\")).convert(\"RGB\")\n",
    "image_array = np.array(image)\n",
    "\n",
    "image_red = Image.open(template.format(color=\"_red\")).convert(\"RGB\")\n",
    "image_array_red = np.array(image_red)\n",
    "\n",
    "image_blue = Image.open(template.format(color=\"_blue\")).convert(\"RGB\")\n",
    "image_array_blue = np.array(image_blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_image = (image_array_red[:, :, 0] >= 250) & (image_array_red[:, :, 1] <= 5) & (image_array_red[:, :, 2] <= 5) & (image_array_blue[:, :, 0] <= 5) & (image_array_blue[:, :, 1] <= 5) & (image_array_blue[:, :, 2] >= 250)\n",
    "is_image_coordinates = np.column_stack(np.where(is_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 119 1160 1259\n"
     ]
    }
   ],
   "source": [
    "print(np.min(is_image_coordinates[:, 0]), np.max(is_image_coordinates[:, 0]), np.min(is_image_coordinates[:, 1]), np.max(is_image_coordinates[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/Cellar/tesseract/5.3.3/bin/tesseract'\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/local/Cellar/tesseract/5.3.3/bin/tesseract'\n",
    "\n",
    "\n",
    "def get_ocr_blocks(image_path):\n",
    "    # This function will use OCR to extract text blocks and their bounding boxes from an image\n",
    "    image = cv2.imread(image_path)\n",
    "    img_h, img_w, _ = image.shape\n",
    "    data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "    blocks = []\n",
    "    for i in range(len(data['text'])):\n",
    "        if int(data['conf'][i]) > 50:  # Consider blocks with confidence > 60%\n",
    "            (x, y, w, h) = (data['left'][i], data['top'][i], data['width'][i], data['height'][i])\n",
    "            text = data['text'][i].strip()\n",
    "            blocks.append({'text': text, 'bbox': (x / img_w, y / img_h, w / img_w, h / img_h)})\n",
    "    return blocks\n",
    "\n",
    "def match_blocks(blocks1, blocks2, v_scale=0.1):\n",
    "    # This function will match blocks between two sets based on text similarity, spatial location, and size similarity\n",
    "    matched_blocks = []\n",
    "    max_distance = (1 + v_scale**2)**0.5\n",
    "    \n",
    "    for block1 in blocks1:\n",
    "        best_match = None\n",
    "        highest_score = 0\n",
    "        \n",
    "        for block2 in blocks2:\n",
    "            # Text similarity\n",
    "            text_similarity = SequenceMatcher(None, block1['text'], block2['text']).ratio()\n",
    "            \n",
    "            if text_similarity > 0.8:  # Text must be similar above a threshold\n",
    "                \n",
    "                # Spatial proximity (normalized by image dimensions for example)\n",
    "                spatial_proximity = 1 - ((block1['bbox'][0] - block2['bbox'][0])**2 + (block1['bbox'][1] * v_scale - block2['bbox'][1] * v_scale)**2)**0.5 / max_distance\n",
    "                \n",
    "                # Size similarity\n",
    "                # size_similarity = 1 - abs(block1['bbox'][2]*block1['bbox'][3] - block2['bbox'][2]*block2['bbox'][3]) / max(block1['bbox'][2]*block1['bbox'][3], block2['bbox'][2]*block2['bbox'][3])\n",
    "\n",
    "                # Combine the scores with weights as needed\n",
    "                # combined_score = (text_similarity * 0.6) + (spatial_proximity * 0.2) + (size_similarity * 0.2)\n",
    "                combined_score = (text_similarity * 0.6) + (spatial_proximity * 0.4)\n",
    "\n",
    "                print(block2)\n",
    "                print(combined_score)\n",
    "\n",
    "                if combined_score > highest_score:\n",
    "                    highest_score = combined_score\n",
    "                    best_match = block2\n",
    "\n",
    "        if best_match:\n",
    "            matched_blocks.append((block1, best_match))\n",
    "        \n",
    "        break\n",
    "    \n",
    "    return matched_blocks\n",
    "\n",
    "\n",
    "def calculate_positional_score(bbox1, bbox2, v_scale=0.1):\n",
    "    max_distance = (1 + v_scale**2)**0.5\n",
    "\n",
    "    # Calculate the Euclidean distance between the center points of two bounding boxes\n",
    "    center1 = (bbox1[0] + bbox1[2] / 2, bbox1[1] + bbox1[3] / 2)\n",
    "    center2 = (bbox2[0] + bbox2[2] / 2, bbox2[1] + bbox2[3] / 2)\n",
    "    distance = ((center1[0] - center2[0]) ** 2 + (center1[1] * v_scale - center2[1] * v_scale) ** 2) ** 0.5\n",
    "    \n",
    "    # Normalize distance based on a predefined max distance, this value could be tuned\n",
    "    normalized_distance = min(distance / max_distance, 1)\n",
    "    \n",
    "    # Calculate score using exponential decay\n",
    "    score = 1 - normalized_distance\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks1 = get_ocr_blocks('./diyi_gpt4.png')\n",
    "blocks2 = get_ocr_blocks('./diyi.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_blocks_by_row(blocks, line_overlap_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Group blocks into rows based on their bounding box y-coordinates.\n",
    "    Blocks that have y-overlapping bounding boxes within a threshold are considered to be on the same row.\n",
    "\n",
    "    :param blocks: List of block dictionaries with 'bbox' as one of the keys.\n",
    "    :param line_overlap_threshold: Threshold for considering blocks to be on the same line (relative to image height).\n",
    "    :return: A list of lists of blocks, with each inner list representing a row.\n",
    "    \"\"\"\n",
    "    # Sort blocks by the top y-coordinate\n",
    "    sorted_blocks = sorted(blocks, key=lambda b: b['bbox'][1])\n",
    "    \n",
    "    rows = []\n",
    "    current_row = []\n",
    "    \n",
    "    for block in sorted_blocks:\n",
    "        # If current_row is empty, start a new row with the current block\n",
    "        if not current_row:\n",
    "            current_row.append(block)\n",
    "        else:\n",
    "            # Compare the current block with the last block in the current row\n",
    "            last_block_in_row = current_row[-1]\n",
    "            # Calculate the vertical overlap between the two blocks\n",
    "            top_y_current = block['bbox'][1]\n",
    "            bottom_y_last = last_block_in_row['bbox'][1] + last_block_in_row['bbox'][3]\n",
    "            vertical_overlap = max(0, bottom_y_last - top_y_current)\n",
    "            \n",
    "            # If there is enough overlap, add the block to the current row\n",
    "            if vertical_overlap > line_overlap_threshold * min(last_block_in_row['bbox'][3], block['bbox'][3]):\n",
    "                current_row.append(block)\n",
    "            else:\n",
    "                # Otherwise, the current block starts a new row\n",
    "                rows.extend(current_row)\n",
    "                current_row = [block]\n",
    "    \n",
    "    # Add the last row if it's not empty\n",
    "    if current_row:\n",
    "        rows.extend(current_row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "grouped_blocks1 = group_blocks_by_row(blocks1)\n",
    "grouped_blocks2 = group_blocks_by_row(blocks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 1316\n"
     ]
    }
   ],
   "source": [
    "print(len(grouped_blocks1), len(grouped_blocks2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching pairs:\n",
      "{'text': 'Diyi', 'bbox': (0.0171875, 0.08472222222222223, 0.03125, 0.03194444444444444)} matched with {'text': 'Diyi', 'bbox': (0.61328125, 0.0079155672823219, 0.040625, 0.005653976630229929)}\n",
      "{'text': 'Yang', 'bbox': (0.0546875, 0.08472222222222223, 0.04375, 0.03194444444444444)} matched with {'text': 'Yang', 'bbox': (0.6609375, 0.0079155672823219, 0.05390625, 0.005653976630229929)}\n",
      "{'text': 'diyiy@stanford.edu', 'bbox': (0.015625, 0.19305555555555556, 0.10625, 0.020833333333333332)} matched with {'text': 'diyiy@cs.stanford.edu', 'bbox': (0.603125, 0.01752732755371278, 0.121875, 0.0028269883151149644)}\n",
      "{'text': 'Computer', 'bbox': (0.01640625, 0.24027777777777778, 0.05390625, 0.020833333333333332)} matched with {'text': 'Computer', 'bbox': (0.57890625, 0.022050508857896722, 0.0546875, 0.0028269883151149644)}\n",
      "{'text': 'Science', 'bbox': (0.07421875, 0.24027777777777778, 0.04375, 0.016666666666666666)} matched with {'text': 'Science', 'bbox': (0.6375, 0.022050508857896722, 0.04140625, 0.0022615906520919715)}\n",
      "{'text': 'Department', 'bbox': (0.12265625, 0.24027777777777778, 0.06484375, 0.020833333333333332)} matched with {'text': 'Department', 'bbox': (0.68359375, 0.022050508857896722, 0.06640625, 0.0028269883151149644)}\n",
      "{'text': 'Natural', 'bbox': (0.01640625, 0.28888888888888886, 0.0390625, 0.016666666666666666)} matched with {'text': 'Natural', 'bbox': (0.5640625, 0.026573690162080662, 0.040625, 0.0022615906520919715)}\n",
      "{'text': 'Language', 'bbox': (0.06015625, 0.28888888888888886, 0.0546875, 0.020833333333333332)} matched with {'text': 'Language', 'bbox': (0.609375, 0.026573690162080662, 0.05390625, 0.0028269883151149644)}\n",
      "{'text': 'Processing', 'bbox': (0.11953125, 0.28888888888888886, 0.06015625, 0.020833333333333332)} matched with {'text': 'Processing', 'bbox': (0.66796875, 0.026573690162080662, 0.05859375, 0.0028269883151149644)}\n",
      "{'text': 'Group', 'bbox': (0.184375, 0.28888888888888886, 0.03359375, 0.020833333333333332)} matched with {'text': 'Group', 'bbox': (0.73125, 0.026573690162080662, 0.03359375, 0.0028269883151149644)}\n",
      "{'text': 'Stanford', 'bbox': (0.01640625, 0.33611111111111114, 0.0453125, 0.016666666666666666)} matched with {'text': 'Stanford', 'bbox': (0.6109375, 0.031096871466264605, 0.046875, 0.0022615906520919715)}\n",
      "{'text': 'University', 'bbox': (0.0671875, 0.33611111111111114, 0.05390625, 0.020833333333333332)} matched with {'text': 'University', 'bbox': (0.6625, 0.031096871466264605, 0.05546875, 0.0028269883151149644)}\n",
      "{'text': 'Google', 'bbox': (0.01640625, 0.38472222222222224, 0.0390625, 0.020833333333333332)} matched with {'text': '[Google', 'bbox': (0.25703125, 0.14794572182434979, 0.05234375, 0.003392385978137957)}\n",
      "{'text': '$2,', 'bbox': (0.06015625, 0.38472222222222224, 0.0171875, 0.019444444444444445)} matched with {'text': ',', 'bbox': (0.2859375, 0.14116094986807387, 0.00078125, 0.0007538635506973238)}\n",
      "{'text': '342', 'bbox': (0.08203125, 0.38472222222222224, 0.01953125, 0.016666666666666666)} matched with {'text': '342', 'bbox': (0.671875, 0.03562005277044855, 0.02109375, 0.0022615906520919715)}\n",
      "{'text': 'Home', 'bbox': (0.01640625, 0.4375, 0.02421875, 0.0125)} matched with {'text': 'Home', 'bbox': (0.2203125, 0.048812664907651716, 0.0328125, 0.0022615906520919715)}\n",
      "{'text': 'Group', 'bbox': (0.059375, 0.4375, 0.025, 0.015277777777777777)} matched with {'text': 'Group', 'bbox': (0.3953125, 0.048812664907651716, 0.03359375, 0.0028269883151149644)}\n",
      "{'text': 'Recent', 'bbox': (0.01640625, 0.5, 0.04609375, 0.018055555555555554)} matched with {'text': 'Recent', 'bbox': (0.15, 0.05974368639276291, 0.05703125, 0.003392385978137957)}\n",
      "{'text': 'Preprints', 'bbox': (0.0671875, 0.5, 0.06015625, 0.02361111111111111)} matched with {'text': 'Preprints', 'bbox': (0.2125, 0.05974368639276291, 0.075, 0.0041462495288352805)}\n",
      "{'text': 'Rehearsal:', 'bbox': (0.01640625, 0.5486111111111112, 0.04140625, 0.0125)} matched with {'text': 'Rehearsal:', 'bbox': (0.18046875, 0.06841311722578214, 0.0578125, 0.0024500565397663023)}\n",
      "{'text': 'Simulating', 'bbox': (0.0640625, 0.5486111111111112, 0.04296875, 0.015277777777777777)} matched with {'text': 'Simulating', 'bbox': (0.2421875, 0.06841311722578214, 0.059375, 0.0028269883151149644)}\n",
      "{'text': 'Chat', 'bbox': (0.11015625, 0.5486111111111112, 0.01953125, 0.0125)} matched with {'text': 'What', 'bbox': (0.228125, 0.18092725216735772, 0.03203125, 0.0024500565397663023)}\n",
      "{'text': 'Facilitate', 'bbox': (0.14375, 0.5486111111111112, 0.0359375, 0.0125)} matched with {'text': 'Kaijie', 'bbox': (0.18046875, 0.11929890689785148, 0.02890625, 0.0028269883151149644)}\n",
      "{'text': 'Resolution', 'bbox': (0.2171875, 0.5486111111111112, 0.04296875, 0.0125)} matched with {'text': 'Resolution', 'bbox': (0.44921875, 0.06841311722578214, 0.05859375, 0.0024500565397663023)}\n",
      "{'text': 'to', 'bbox': (0.1328125, 0.55, 0.00703125, 0.011111111111111112)} matched with {'text': 'to', 'bbox': (0.3515625, 0.0687900490011308, 0.0109375, 0.0020731247644176403)}\n",
      "{'text': 'Using', 'bbox': (0.50703125, 0.575, 0.02265625, 0.015277777777777777)} matched with {'text': 'Using', 'bbox': (0.18046875, 0.15642668676969468, 0.03125, 0.0028269883151149644)}\n",
      "{'text': 'Large', 'bbox': (0.53359375, 0.575, 0.02265625, 0.015277777777777777)} matched with {'text': 'Large', 'bbox': (0.59375, 0.2272898605352431, 0.03046875, 0.0028269883151149644)}\n",
      "{'text': 'Language', 'bbox': (0.56015625, 0.575, 0.040625, 0.015277777777777777)} matched with {'text': 'Language', 'bbox': (0.628125, 0.2272898605352431, 0.0546875, 0.0028269883151149644)}\n",
      "{'text': 'Models', 'bbox': (0.6046875, 0.575, 0.02890625, 0.0125)} matched with {'text': 'Models', 'bbox': (0.68671875, 0.2272898605352431, 0.040625, 0.0024500565397663023)}\n",
      "{'text': 'Psychology', 'bbox': (0.646875, 0.575, 0.04765625, 0.015277777777777777)} matched with {'text': 'Psychology,', 'bbox': (0.27265625, 0.17451941198643045, 0.06484375, 0.0028269883151149644)}\n",
      "{'text': 'in', 'bbox': (0.63671875, 0.5777777777777777, 0.00625, 0.009722222222222222)} matched with {'text': 'in', 'bbox': (0.45703125, 0.49905767056162836, 0.009375, 0.0022615906520919715)}\n",
      "{'text': 'Publications', 'bbox': (0.01640625, 0.6180555555555556, 0.08203125, 0.018055555555555554)} matched with {'text': 'Publications', 'bbox': (0.5546875, 0.0469280060309084, 0.0671875, 0.00603090840557859)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def calculate_similarity(block1, block2, max_distance=1.42):\n",
    "    text_similarity = SequenceMatcher(None, block1['text'], block2['text']).ratio()\n",
    "    spatial_proximity = 1 - ((block1['bbox'][0] - block2['bbox'][0])**2 + (block1['bbox'][1] - block2['bbox'][1])**2)**0.5 / max_distance\n",
    "    combined_score = (text_similarity * 1.0) + (spatial_proximity * 0.0)\n",
    "    return combined_score\n",
    "\n",
    "def adjust_cost_for_context(cost_matrix, consecutive_bonus=1.0, window_size=20):\n",
    "    n, m = cost_matrix.shape\n",
    "    adjusted_cost_matrix = np.copy(cost_matrix)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            bonus = 0\n",
    "            if adjusted_cost_matrix[i][j] >= -0.5:\n",
    "                continue\n",
    "            # Check left context\n",
    "            for k in range(1, window_size + 1):\n",
    "                if i >= k and j >= k:\n",
    "                    # bonus += consecutive_bonus * (cost_matrix[i-k, j-k] < 0)\n",
    "                    bonus += consecutive_bonus * cost_matrix[i-k, j-k]\n",
    "            # Check right context\n",
    "            for k in range(1, window_size + 1):\n",
    "                if i + k < n and j + k < m:\n",
    "                    # bonus += consecutive_bonus * (cost_matrix[i+k, j+k] < 0\n",
    "                    bonus += consecutive_bonus * cost_matrix[i+k, j+k]\n",
    "            adjusted_cost_matrix[i][j] += bonus\n",
    "    return adjusted_cost_matrix\n",
    "\n",
    "def create_cost_matrix(A, B):\n",
    "    n = len(A)\n",
    "    m = len(B)\n",
    "    cost_matrix = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            cost_matrix[i, j] = -calculate_similarity(A[i], B[j])\n",
    "    return cost_matrix\n",
    "\n",
    "def find_maximum_matching(A, B, consecutive_bonus, window_size):\n",
    "    cost_matrix = create_cost_matrix(A, B)\n",
    "    cost_matrix = adjust_cost_for_context(cost_matrix, consecutive_bonus, window_size)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    return list(zip(row_ind, col_ind))\n",
    "\n",
    "# Example usage\n",
    "A = grouped_blocks1\n",
    "B = grouped_blocks2\n",
    "\n",
    "matching = find_maximum_matching(A, B, 0.1, 2)\n",
    "matched_list = []\n",
    "print(\"Matching pairs:\")\n",
    "for i, j in matching:\n",
    "    print(f\"{A[i]} matched with {B[j]}\")\n",
    "    matched_list.append([A[i]['bbox'], B[j]['bbox']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def draw_matched_bboxes(img1, img2, matched_bboxes):\n",
    "    # Create copies of images to draw on\n",
    "    img1_drawn = img1.copy()\n",
    "    img2_drawn = img2.copy()\n",
    "\n",
    "    h1, w1, _ = img1.shape\n",
    "    h2, w2, _ = img2.shape\n",
    "    \n",
    "\n",
    "    # Iterate over matched bounding boxes\n",
    "    for bbox_pair in matched_bboxes:\n",
    "        # Random color for each pair\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "        # Ensure that the bounding box coordinates are integers\n",
    "        bbox1 = [int(bbox_pair[0][0] * w1), int(bbox_pair[0][1] * h1), int(bbox_pair[0][2] * w1), int(bbox_pair[0][3] * h1)]\n",
    "        bbox2 = [int(bbox_pair[1][0] * w2), int(bbox_pair[1][1] * h2), int(bbox_pair[1][2] * w2), int(bbox_pair[1][3] * h2)]\n",
    "\n",
    "        # Draw bbox on the first image\n",
    "        top_left_1 = (bbox1[0], bbox1[1])\n",
    "        bottom_right_1 = (bbox1[0] + bbox1[2], bbox1[1] + bbox1[3])\n",
    "        img1_drawn = cv2.rectangle(img1_drawn, top_left_1, bottom_right_1, color, 2)\n",
    "\n",
    "        # Draw bbox on the second image\n",
    "        top_left_2 = (bbox2[0], bbox2[1])\n",
    "        bottom_right_2 = (bbox2[0] + bbox2[2], bbox2[1] + bbox2[3])\n",
    "        img2_drawn = cv2.rectangle(img2_drawn, top_left_2, bottom_right_2, color, 2)\n",
    "\n",
    "    return img1_drawn, img2_drawn\n",
    "\n",
    "img1 = cv2.imread('./diyi_gpt4.png')\n",
    "img2 = cv2.imread('./diyi.png')\n",
    "\n",
    "img1_with_boxes, img2_with_boxes = draw_matched_bboxes(img1, img2, matched_list)\n",
    "\n",
    "cv2.imwrite('./diyi_gpt4_demo.png', img1_with_boxes)\n",
    "cv2.imwrite('./diyi_demo.png', img2_with_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR, draw_ocr\n",
    "\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang=\"en\")  # need to run only once to download and load model into memory\n",
    "img_path = './diyi_gpt4.png'\n",
    "result = ocr.ocr(img_path, cls=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
