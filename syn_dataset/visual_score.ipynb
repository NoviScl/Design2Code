{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/Cellar/tesseract/5.3.3/bin/tesseract'\n",
    "\n",
    "def mask_text_cv2(cv2_image):\n",
    "    # Convert the cv2 image (BGR) to PIL Image (RGB)\n",
    "    rgb_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_image)\n",
    "\n",
    "    # Use pytesseract to do OCR on the image\n",
    "    text_data = pytesseract.image_to_data(pil_image)\n",
    "\n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    print(text_data.split('\\n')[0])\n",
    "\n",
    "    # Process the OCR data\n",
    "    for line in text_data.split('\\n')[1:]:\n",
    "        if line.strip() == '':\n",
    "            continue\n",
    "\n",
    "        parts = line.split()\n",
    "        print(parts)\n",
    "        if len(parts) >= 12:\n",
    "            x, y, width, height = map(int, parts[6:10])\n",
    "            # Draw a white rectangle over the detected text\n",
    "            draw.rectangle([x, y, x + width, y + height], fill=\"white\")\n",
    "\n",
    "    # Convert PIL Image back to cv2 format (BGR)\n",
    "    masked_cv2_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "    return masked_cv2_image\n",
    "\n",
    "\n",
    "def check_match_images(src_img, web_img, visualize=False):\n",
    "    # Read the images\n",
    "    image_b = cv2.imread(web_img)\n",
    "    image_b = mask_text_cv2(image_b)\n",
    "    image_a = cv2.imread(src_img)\n",
    "\n",
    "    # SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Find keypoints and descriptors\n",
    "    keypoints_a, descriptors_a = sift.detectAndCompute(image_a, None)\n",
    "    keypoints_b, descriptors_b = sift.detectAndCompute(image_b, None)\n",
    "\n",
    "    # FLANN based matcher\n",
    "    index_params = dict(algorithm=1, trees=5)\n",
    "    search_params = dict()\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = flann.knnMatch(descriptors_a, descriptors_b, k=2)\n",
    "\n",
    "    # Keep good matches: Lowe's ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    if len(good_matches) > 10: # adjust this threshold\n",
    "\n",
    "        image_matches = cv2.drawMatches(image_a, keypoints_a, image_b, keypoints_b, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    \n",
    "        src_pts = np.float32([keypoints_a[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([keypoints_b[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Find homography\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        # Use the homography matrix M to transform the corners of Image A to Image B's plane\n",
    "        h, w = image_a.shape[:2]\n",
    "        pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "        dst = cv2.perspectiveTransform(pts, M)\n",
    "\n",
    "        # Draw the transformed image on Image B\n",
    "        image_b_with_a = cv2.polylines(image_b, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "        # gray = cv2.cvtColor(image_b_with_a, cv2.COLOR_BGR2GRAY)\n",
    "        if visualize:\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            ax.axis('off')\n",
    "            plt.imshow(image_matches)\n",
    "            plt.show()\n",
    "\n",
    "        hb, wb = image_b.shape[:2]\n",
    "        print(hb, wb)\n",
    "        print(dst)\n",
    "\n",
    "        # Extract scale and translation (approximate)\n",
    "        scale_x = np.linalg.norm(dst[1] - dst[0]) / hb\n",
    "        scale_y = np.linalg.norm(dst[2] - dst[1]) / wb\n",
    "        translation = dst[0][0] / np.array([hb, wb])\n",
    "\n",
    "        print(f\"Relative height: {scale_x}, Relative width: {scale_y}\")\n",
    "        print(f\"Top-Left Corner Coordinate: {translation}\")\n",
    "        return scale_x, scale_y, translation.tolist()\n",
    "    else:\n",
    "        print(\"Image not found!\")\n",
    "        return None, None, [None, None]\n",
    "    \n",
    "\n",
    "# check_match_images('../trial_dataset/rick.jpg', './diyi.png')\n",
    "check_match_images('../trial_dataset/rick.jpg', './diyi_gpt4.png', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./diyi_gpt4{color}.png\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"./diyi_gpt4.png\"\n",
    "template = file_path[:-4] + \"{color}\" + file_path[-4:]\n",
    "\n",
    "image = Image.open(template.format(color=\"\")).convert(\"RGB\")\n",
    "image_array = np.array(image)\n",
    "\n",
    "image_red = Image.open(template.format(color=\"_red\")).convert(\"RGB\")\n",
    "image_array_red = np.array(image_red)\n",
    "\n",
    "image_blue = Image.open(template.format(color=\"_blue\")).convert(\"RGB\")\n",
    "image_array_blue = np.array(image_blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_image = (image_array_red[:, :, 0] >= 250) & (image_array_red[:, :, 1] <= 5) & (image_array_red[:, :, 2] <= 5) & (image_array_blue[:, :, 0] <= 5) & (image_array_blue[:, :, 1] <= 5) & (image_array_blue[:, :, 2] >= 250)\n",
    "is_image_coordinates = np.column_stack(np.where(is_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 119 1160 1259\n"
     ]
    }
   ],
   "source": [
    "print(np.min(is_image_coordinates[:, 0]), np.max(is_image_coordinates[:, 0]), np.min(is_image_coordinates[:, 1]), np.max(is_image_coordinates[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import SequenceMatcher\n",
    "import pytesseract\n",
    "import cv2\n",
    "\n",
    "# pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/Cellar/tesseract/5.3.3/bin/tesseract'\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/local/Cellar/tesseract/5.3.3/bin/tesseract'\n",
    "\n",
    "\n",
    "def get_ocr_blocks(image_path):\n",
    "    # This function will use OCR to extract text blocks and their bounding boxes from an image\n",
    "    image = cv2.imread(image_path)\n",
    "    img_h, img_w, _ = image.shape\n",
    "    data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "    blocks = []\n",
    "    for i in range(len(data['text'])):\n",
    "        if int(data['conf'][i]) > 0:  # Consider blocks with confidence > 60%\n",
    "            (x, y, w, h) = (data['left'][i], data['top'][i], data['width'][i], data['height'][i])\n",
    "            text = data['text'][i].strip()\n",
    "            if len(text) == 0:\n",
    "                continue\n",
    "            blocks.append({'text': text, 'bbox': (x / img_w, y / img_h, w / img_w, h / img_h)})\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def match_blocks(blocks1, blocks2, v_scale=0.1):\n",
    "    # This function will match blocks between two sets based on text similarity, spatial location, and size similarity\n",
    "    matched_blocks = []\n",
    "    max_distance = (1 + v_scale**2)**0.5\n",
    "    \n",
    "    for block1 in blocks1:\n",
    "        best_match = None\n",
    "        highest_score = 0\n",
    "        \n",
    "        for block2 in blocks2:\n",
    "            # Text similarity\n",
    "            text_similarity = SequenceMatcher(None, block1['text'], block2['text']).ratio()\n",
    "            \n",
    "            if text_similarity > 0.8:  # Text must be similar above a threshold\n",
    "                \n",
    "                # Spatial proximity (normalized by image dimensions for example)\n",
    "                spatial_proximity = 1 - ((block1['bbox'][0] - block2['bbox'][0])**2 + (block1['bbox'][1] * v_scale - block2['bbox'][1] * v_scale)**2)**0.5 / max_distance\n",
    "                \n",
    "                # Size similarity\n",
    "                # size_similarity = 1 - abs(block1['bbox'][2]*block1['bbox'][3] - block2['bbox'][2]*block2['bbox'][3]) / max(block1['bbox'][2]*block1['bbox'][3], block2['bbox'][2]*block2['bbox'][3])\n",
    "\n",
    "                # Combine the scores with weights as needed\n",
    "                # combined_score = (text_similarity * 0.6) + (spatial_proximity * 0.2) + (size_similarity * 0.2)\n",
    "                combined_score = (text_similarity * 0.6) + (spatial_proximity * 0.4)\n",
    "\n",
    "                print(block2)\n",
    "                print(combined_score)\n",
    "\n",
    "                if combined_score > highest_score:\n",
    "                    highest_score = combined_score\n",
    "                    best_match = block2\n",
    "\n",
    "        if best_match:\n",
    "            matched_blocks.append((block1, best_match))\n",
    "        \n",
    "        break\n",
    "    \n",
    "    return matched_blocks\n",
    "\n",
    "\n",
    "def calculate_positional_score(bbox1, bbox2, v_scale=0.1):\n",
    "    max_distance = (1 + v_scale**2)**0.5\n",
    "\n",
    "    # Calculate the Euclidean distance between the center points of two bounding boxes\n",
    "    center1 = (bbox1[0] + bbox1[2] / 2, bbox1[1] + bbox1[3] / 2)\n",
    "    center2 = (bbox2[0] + bbox2[2] / 2, bbox2[1] + bbox2[3] / 2)\n",
    "    distance = ((center1[0] - center2[0]) ** 2 + (center1[1] * v_scale - center2[1] * v_scale) ** 2) ** 0.5\n",
    "    \n",
    "    # Normalize distance based on a predefined max distance, this value could be tuned\n",
    "    normalized_distance = min(distance / max_distance, 1)\n",
    "    \n",
    "    # Calculate score using exponential decay\n",
    "    score = 1 - normalized_distance\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks1 = get_ocr_blocks('./diyi_gpt4.png')\n",
    "blocks2 = get_ocr_blocks('./diyi.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_blocks_by_row(blocks, line_overlap_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Group blocks into rows based on their bounding box y-coordinates.\n",
    "    Blocks that have y-overlapping bounding boxes within a threshold are considered to be on the same row.\n",
    "\n",
    "    :param blocks: List of block dictionaries with 'bbox' as one of the keys.\n",
    "    :param line_overlap_threshold: Threshold for considering blocks to be on the same line (relative to image height).\n",
    "    :return: A list of lists of blocks, with each inner list representing a row.\n",
    "    \"\"\"\n",
    "    # Sort blocks by the top y-coordinate\n",
    "    sorted_blocks = sorted(blocks, key=lambda b: b['bbox'][1])\n",
    "    \n",
    "    rows = []\n",
    "    current_row = []\n",
    "    \n",
    "    for block in sorted_blocks:\n",
    "        # If current_row is empty, start a new row with the current block\n",
    "        if not current_row:\n",
    "            current_row.append(block)\n",
    "        else:\n",
    "            # Compare the current block with the last block in the current row\n",
    "            last_block_in_row = current_row[-1]\n",
    "            # Calculate the vertical overlap between the two blocks\n",
    "            top_y_current = block['bbox'][1]\n",
    "            bottom_y_last = last_block_in_row['bbox'][1] + last_block_in_row['bbox'][3]\n",
    "            vertical_overlap = max(0, bottom_y_last - top_y_current)\n",
    "            \n",
    "            # If there is enough overlap, add the block to the current row\n",
    "            if vertical_overlap > line_overlap_threshold * min(last_block_in_row['bbox'][3], block['bbox'][3]):\n",
    "                current_row.append(block)\n",
    "            else:\n",
    "                # Otherwise, the current block starts a new row\n",
    "                current_row.sort(key=lambda b: (b['bbox'][0]))\n",
    "                rows.extend(current_row)\n",
    "                current_row = [block]\n",
    "    \n",
    "    # Add the last row if it's not empty\n",
    "    if current_row:\n",
    "        current_row.sort(key=lambda b: (b['bbox'][0]))\n",
    "        rows.extend(current_row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "def merge_blocks(blocks, line_overlap_threshold=1.5, avg_char_space_ratio=2):\n",
    "    # Sort blocks by their y-coordinate and then by their x-coordinate\n",
    "    blocks = group_blocks_by_row(blocks)\n",
    "    \n",
    "    merged_blocks = []\n",
    "    last_block = None\n",
    "\n",
    "    for block in blocks:\n",
    "        if last_block is not None:\n",
    "            # Check vertical overlap; if the y difference is smaller than the height of the block, there is an overlap\n",
    "            y_diff = abs(block['bbox'][1] + block['bbox'][3] - last_block['bbox'][1])\n",
    "            height = max(last_block['bbox'][3], block['bbox'][3])\n",
    "\n",
    "            # Estimate average character width for the last block\n",
    "            last_block_char_count = max(len(last_block['text'].strip()), 1)  # Avoid division by zero\n",
    "            avg_char_width_last_block = last_block['bbox'][2] / last_block_char_count\n",
    "\n",
    "            # Estimate the expected space if there was a single space between the two blocks\n",
    "            expected_space_width = avg_char_width_last_block * avg_char_space_ratio\n",
    "\n",
    "            # Calculate actual horizontal gap\n",
    "            right_edge_last_block = last_block['bbox'][0] + last_block['bbox'][2]\n",
    "            actual_gap = block['bbox'][0] - right_edge_last_block\n",
    "            \n",
    "            # Check if blocks are on the same line and the gap is about the width of a space or less\n",
    "            if y_diff < height * line_overlap_threshold and actual_gap <= expected_space_width:\n",
    "                # Merge the text and extend the bbox\n",
    "                merged_text = f\"{last_block['text']} {block['text']}\"\n",
    "                merged_bbox = (\n",
    "                    last_block['bbox'][0],  # x-coordinate remains the same\n",
    "                    min(last_block['bbox'][1], block['bbox'][1]),  # y-coordinate is the upper one\n",
    "                    right_edge_last_block - last_block['bbox'][0] + actual_gap + block['bbox'][2],  # width\n",
    "                    max(last_block['bbox'][3], block['bbox'][3])  # height is the taller one\n",
    "                )\n",
    "                last_block = {'text': merged_text, 'bbox': merged_bbox}\n",
    "            else:\n",
    "                # No merge, add the last block to the list\n",
    "                merged_blocks.append(last_block)\n",
    "                last_block = block\n",
    "        else:\n",
    "            last_block = block\n",
    "\n",
    "    # Don't forget to add the last block\n",
    "    if last_block:\n",
    "        merged_blocks.append(last_block)\n",
    "\n",
    "    return merged_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 191\n"
     ]
    }
   ],
   "source": [
    "blocks1 = merge_blocks(blocks1)\n",
    "blocks2 = merge_blocks(blocks2)\n",
    "print(len(blocks1), len(blocks2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching pairs:\n",
      "{'text': 'Diyi Yang', 'bbox': (0.0171875, 0.08472222222222223, 0.08124999999999999, 0.03194444444444444)} matched with {'text': 'Diyi Yang', 'bbox': (0.61328125, 0.0079155672823219, 0.10156249999999996, 0.005653976630229929)}\n",
      "{'text': 'diyiy@stanford.edu', 'bbox': (0.015625, 0.19305555555555556, 0.10625, 0.020833333333333332)} matched with {'text': 'diyiy@cs.stanford.edu', 'bbox': (0.603125, 0.01752732755371278, 0.121875, 0.0028269883151149644)}\n",
      "{'text': 'Computer Science Department', 'bbox': (0.01640625, 0.24027777777777778, 0.17109375, 0.020833333333333332)} matched with {'text': 'Computer Science Department', 'bbox': (0.57890625, 0.022050508857896722, 0.17109375000000004, 0.0028269883151149644)}\n",
      "{'text': 'Natural Language Processing Group', 'bbox': (0.01640625, 0.28888888888888886, 0.2015625, 0.020833333333333332)} matched with {'text': 'Natural Language Processing Group', 'bbox': (0.5640625, 0.026573690162080662, 0.20078124999999994, 0.0028269883151149644)}\n",
      "{'text': 'Stanford University', 'bbox': (0.01640625, 0.33611111111111114, 0.1046875, 0.020833333333333332)} matched with {'text': 'Stanford University', 'bbox': (0.6109375, 0.031096871466264605, 0.10703124999999995, 0.0028269883151149644)}\n",
      "{'text': 'Google $2, 342', 'bbox': (0.01640625, 0.38472222222222224, 0.08515625, 0.020833333333333332)} matched with {'text': 'Gates 342', 'bbox': (0.6359375, 0.03562005277044855, 0.05703124999999996, 0.0022615906520919715)}\n",
      "{'text': 'Home', 'bbox': (0.01640625, 0.4375, 0.02421875, 0.0125)} matched with {'text': 'Home', 'bbox': (0.2203125, 0.048812664907651716, 0.0328125, 0.0022615906520919715)}\n",
      "{'text': 'Group', 'bbox': (0.059375, 0.4375, 0.025, 0.015277777777777777)} matched with {'text': 'Group', 'bbox': (0.3953125, 0.048812664907651716, 0.03359375, 0.0028269883151149644)}\n",
      "{'text': 'Publications', 'bbox': (0.10390625, 0.4375, 0.04921875, 0.0125)} matched with {'text': 'Publications [Google Scholar] [Show Selected] [Show All]', 'bbox': (0.15, 0.1473803241613268, 0.41015625, 0.003392385978137957)}\n",
      "{'text': 'Teaching', 'bbox': (0.171875, 0.4375, 0.03671875, 0.016666666666666666)} matched with {'text': 'Teaching', 'bbox': (0.7390625, 0.048812664907651716, 0.0484375, 0.0028269883151149644)}\n",
      "{'text': 'Recent Preprints', 'bbox': (0.01640625, 0.5, 0.1109375, 0.02361111111111111)} matched with {'text': 'Recent Preprints', 'bbox': (0.15, 0.05974368639276291, 0.1375, 0.0041462495288352805)}\n",
      "{'text': 'Rehearsal: Simulating Chat to Facilitate Conflict Resolution', 'bbox': (0.01640625, 0.5486111111111112, 0.24375, 0.015277777777777777)} matched with {'text': '* Rehearsal: Simulating Conflict to Teach Conflict Resolution', 'bbox': (0.16875, 0.06841311722578214, 0.3390625, 0.0028269883151149644)}\n",
      "{'text': 'Using Large Language Models in Psychology', 'bbox': (0.50703125, 0.575, 0.18749999999999994, 0.015277777777777777)} matched with {'text': '* Using Large Language Models in Psychology', 'bbox': (0.16875, 0.15642668676969468, 0.26171875, 0.0028269883151149644)}\n",
      "{'text': 'Publications', 'bbox': (0.01640625, 0.6180555555555556, 0.08203125, 0.018055555555555554)} matched with {'text': 'Publications', 'bbox': (0.5546875, 0.0469280060309084, 0.0671875, 0.00603090840557859)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def calculate_similarity(block1, block2, max_distance=1.42):\n",
    "    text_similarity = SequenceMatcher(None, block1['text'], block2['text']).ratio()\n",
    "    spatial_proximity = 1 - ((block1['bbox'][0] - block2['bbox'][0])**2 + (block1['bbox'][1] - block2['bbox'][1])**2)**0.5 / max_distance\n",
    "    combined_score = (text_similarity * 1.0) + (spatial_proximity * 0.0)\n",
    "    return combined_score\n",
    "\n",
    "def adjust_cost_for_context(cost_matrix, consecutive_bonus=1.0, window_size=20):\n",
    "    n, m = cost_matrix.shape\n",
    "    adjusted_cost_matrix = np.copy(cost_matrix)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            bonus = 0\n",
    "            if adjusted_cost_matrix[i][j] >= -0.5:\n",
    "                continue\n",
    "            nearby_matrix = cost_matrix[max(0, i - window_size):min(n, i + window_size), max(0, j - window_size):min(m, j + window_size)]\n",
    "            flattened_array = nearby_matrix.flatten()\n",
    "            sorted_array = np.sort(flattened_array)[::-1]\n",
    "            sorted_array = np.delete(sorted_array, np.where(sorted_array == cost_matrix[i, j])[0][0])\n",
    "            top_k_elements = sorted_array[:window_size]\n",
    "            sum_top_k = np.sum(top_k_elements)\n",
    "            bonus = consecutive_bonus * sum_top_k\n",
    "            \"\"\"\n",
    "            # Check left context\n",
    "            for k in range(1, window_size + 1):\n",
    "                if i >= k and j >= k:\n",
    "                    # bonus += consecutive_bonus * (cost_matrix[i-k, j-k] < 0)\n",
    "                    bonus += consecutive_bonus * cost_matrix[i-k, j-k]\n",
    "            # Check right context\n",
    "            for k in range(1, window_size + 1):\n",
    "                if i + k < n and j + k < m:\n",
    "                    # bonus += consecutive_bonus * (cost_matrix[i+k, j+k] < 0\n",
    "                    bonus += consecutive_bonus * cost_matrix[i+k, j+k]\n",
    "            \"\"\"\n",
    "            adjusted_cost_matrix[i][j] += bonus\n",
    "    return adjusted_cost_matrix\n",
    "\n",
    "def create_cost_matrix(A, B):\n",
    "    n = len(A)\n",
    "    m = len(B)\n",
    "    cost_matrix = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            cost_matrix[i, j] = -calculate_similarity(A[i], B[j])\n",
    "    return cost_matrix\n",
    "\n",
    "def find_maximum_matching(A, B, consecutive_bonus, window_size):\n",
    "    cost_matrix = create_cost_matrix(A, B)\n",
    "    cost_matrix = adjust_cost_for_context(cost_matrix, consecutive_bonus, window_size)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    return list(zip(row_ind, col_ind))\n",
    "\n",
    "# Example usage\n",
    "A = blocks1\n",
    "B = blocks2\n",
    "\n",
    "matching = find_maximum_matching(A, B, 1.0, 5)\n",
    "matched_list = []\n",
    "print(\"Matching pairs:\")\n",
    "for i, j in matching:\n",
    "    print(f\"{A[i]} matched with {B[j]}\")\n",
    "    matched_list.append([A[i]['bbox'], B[j]['bbox']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def draw_matched_bboxes(img1, img2, matched_bboxes):\n",
    "    # Create copies of images to draw on\n",
    "    img1_drawn = img1.copy()\n",
    "    img2_drawn = img2.copy()\n",
    "\n",
    "    h1, w1, _ = img1.shape\n",
    "    h2, w2, _ = img2.shape\n",
    "    \n",
    "\n",
    "    # Iterate over matched bounding boxes\n",
    "    for bbox_pair in matched_bboxes:\n",
    "        # Random color for each pair\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "        # Ensure that the bounding box coordinates are integers\n",
    "        bbox1 = [int(bbox_pair[0][0] * w1), int(bbox_pair[0][1] * h1), int(bbox_pair[0][2] * w1), int(bbox_pair[0][3] * h1)]\n",
    "        bbox2 = [int(bbox_pair[1][0] * w2), int(bbox_pair[1][1] * h2), int(bbox_pair[1][2] * w2), int(bbox_pair[1][3] * h2)]\n",
    "\n",
    "        # Draw bbox on the first image\n",
    "        top_left_1 = (bbox1[0], bbox1[1])\n",
    "        bottom_right_1 = (bbox1[0] + bbox1[2], bbox1[1] + bbox1[3])\n",
    "        img1_drawn = cv2.rectangle(img1_drawn, top_left_1, bottom_right_1, color, 2)\n",
    "\n",
    "        # Draw bbox on the second image\n",
    "        top_left_2 = (bbox2[0], bbox2[1])\n",
    "        bottom_right_2 = (bbox2[0] + bbox2[2], bbox2[1] + bbox2[3])\n",
    "        img2_drawn = cv2.rectangle(img2_drawn, top_left_2, bottom_right_2, color, 2)\n",
    "\n",
    "    return img1_drawn, img2_drawn\n",
    "\n",
    "img1 = cv2.imread('./diyi_gpt4.png')\n",
    "img2 = cv2.imread('./diyi.png')\n",
    "\n",
    "img1_with_boxes, img2_with_boxes = draw_matched_bboxes(img1, img2, matched_list)\n",
    "\n",
    "cv2.imwrite('./diyi_gpt4_demo.png', img1_with_boxes)\n",
    "cv2.imwrite('./diyi_demo.png', img2_with_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
